{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiE2AeYaKn+nOXUEFNR4du",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyoungg/Colab_stuff/blob/main/full_random_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Predict each of these variables independently MAJOR_CASH, MAJOR_PLEDGE, COMMIT_MAJOR, INFLATION_MAJOR_COMMIT, some of the data may also require conversion from qualitative variables. By independent I mean I would like to not consider MAJOR_CASH, MAJOR_PLEDGE, COMMIT_MAJOR, INFLATION_MAJOR_COMMIT as part of the entire dataset\n",
        "\n",
        "Here is all the data you need:\n",
        "/tmp/Complete_Randomized_Dataset.csv"
      ],
      "metadata": {
        "id": "1WZRP7JL7_Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) data_loading\n",
        "\n",
        "### Subtask:\n",
        "Load the data from the provided CSV file into a pandas DataFrame."
      ],
      "metadata": {
        "id": "qjKFexPT8WKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan\n",
        "\n",
        "1. **data_loading**: Load the data SQL into a pandas DataFrame using `pd.read_csv()`.\n",
        "2. **data_exploration**:\n",
        "    - Use `df.info()` and `df.describe()` to understand the data types and distributions of each variable, including `MAJOR_CASH`, `MAJOR_PLEDGE`, `COMMIT_MAJOR`, and `INFLATION_MAJOR_COMMIT`.\n",
        "    - Identify qualitative variables that need conversion by checking their data types.\n",
        "3. **data_preparation**: Create four separate copies of the DataFrame using `df.copy()` for predicting each target variable (`MAJOR_CASH`, `MAJOR_PLEDGE`, `COMMIT_MAJOR`, `INFLATION_MAJOR_COMMIT`).\n",
        "4. **data_wrangling**:\n",
        "    - In each DataFrame copy, drop the other three target variables using `df.drop(columns=['column_name'])`.\n",
        "    - Convert qualitative variables into numerical representations using appropriate encoding techniques (e.g., `pd.get_dummies()` for one-hot encoding) within each DataFrame copy.\n",
        "5. **feature_engineering**:\n",
        "    - For each DataFrame copy, select relevant features for prediction. This may involve dropping irrelevant columns using `df.drop(columns=['column_name'])` or creating new features based on existing ones using Python 3 syntax.\n",
        "    - Handle missing values in the features using appropriate techniques (e.g., `df.fillna()` for imputation) within each DataFrame copy.\n",
        "6. **data_splitting**: For each DataFrame copy, split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`.\n",
        "7. **model_training**: Train a separate machine learning model (e.g., regression, classification) for each target variable using its corresponding DataFrame copy. Choose the model based on the nature of the target variable (continuous or categorical). Import necessary models from `sklearn` using Python 3 syntax.\n",
        "    - **model_training**: Train a model to predict `MAJOR_CASH` using the first DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `MAJOR_PLEDGE` using the second DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `COMMIT_MAJOR` using the third DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `INFLATION_MAJOR_COMMIT` using the fourth DataFrame copy.\n",
        "8. **model_evaluation**: Evaluate the performance of each model on its corresponding testing set using appropriate metrics (e.g., R-squared, accuracy, precision, recall). Import necessary metrics from `sklearn.metrics` using Python 3 syntax.\n",
        "    - **model_evaluation**: Evaluate the `MAJOR_CASH` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `MAJOR_PLEDGE` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `COMMIT_MAJOR` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `INFLATION_MAJOR_COMMIT` prediction model.\n",
        "9. **finish_task**: Write a summary report describing the process, the models trained for each target variable, their performance, and any insights gained from the analysis. Include recommendations for future work."
      ],
      "metadata": {
        "id": "_fuZlgQw8NOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install necessary packages"
      ],
      "metadata": {
        "id": "x2OOy6mM8t00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install    faker"
      ],
      "metadata": {
        "id": "Wq6x0jf8rfnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Random test data"
      ],
      "metadata": {
        "id": "CG_QFfYI9Fft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker and constants\n",
        "fake = Faker()\n",
        "num_rows = 10000\n",
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\",\n",
        "    \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\",\n",
        "    \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\",\n",
        "    \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n",
        "    \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "# Generate random data for all fields\n",
        "data = {\n",
        "    \"MAJOR_CASH\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MAJOR_PLEDGE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COMMIT_MAJOR\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"INFLATION_MAJOR_COMMIT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_COMMIT_VALUE\": [random.uniform(1000, 100000) for _ in range(num_rows)],\n",
        "    \"BASELINE\": [random.uniform(500, 50000) for _ in range(num_rows)],\n",
        "    \"PRINCIPAL_GIFT\": [random.uniform(100, 10000) for _ in range(num_rows)],\n",
        "    \"BELOW_BASELINE\": [random.uniform(0, 5000) for _ in range(num_rows)],\n",
        "    \"LEADERSHIP_GIFT\": [random.uniform(5000, 50000) for _ in range(num_rows)],\n",
        "    \"BASELINE_NO_LEADERSHIP\": [random.uniform(500, 5000) for _ in range(num_rows)],\n",
        "    \"LEGAL_CREDIT\": [random.uniform(100, 100000) for _ in range(num_rows)],\n",
        "    \"CASH\": [random.uniform(0, 50000) for _ in range(num_rows)],\n",
        "    \"PLEDGE\": [random.uniform(0, 30000) for _ in range(num_rows)],\n",
        "    \"DEFERRED\": [random.uniform(0, 10000) for _ in range(num_rows)],\n",
        "    \"CASH_RECEIVED\": [random.uniform(0, 50000) for _ in range(num_rows)],\n",
        "    \"OUTSTANDING_BALANCE\": [random.uniform(0, 20000) for _ in range(num_rows)],\n",
        "    \"NON_GIFT\": [random.uniform(0, 10000) for _ in range(num_rows)],\n",
        "    \"DATE_FIRST_GIFT\": [fake.date_between(start_date=\"-30y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"FIRST_GIFT_AMOUNT\": [random.uniform(50, 10000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJUSTED_FIRST_AMOUNT\": [random.uniform(50, 15000) for _ in range(num_rows)],\n",
        "    \"DATE_LAST_GIFT\": [fake.date_between(start_date=\"-5y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_AMOUNT\": [random.uniform(50, 10000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJ_LAST_AMOUNT\": [random.uniform(50, 15000) for _ in range(num_rows)],\n",
        "    \"LARGEST_DATE\": [fake.date_between(start_date=\"-10y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LARGEST_AMOUNT\": [random.uniform(100, 20000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJ_LARGEST_AMOUNT\": [random.uniform(100, 25000) for _ in range(num_rows)],\n",
        "    \"PRIMARY_UNIT\": [f\"Unit_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"PRIMARY_UNIT_LIFETIME_FUNDRAISING\": [random.uniform(10000, 500000) for _ in range(num_rows)],\n",
        "    \"TOTAL_YEARS_GIVING\": [random.randint(1, 40) for _ in range(num_rows)],\n",
        "    \"RECENT_CONSECUTIVE_STREAK_GIVING\": [random.randint(1, 10) for _ in range(num_rows)],\n",
        "    \"SEGMENT\": [f\"Segment_{random.randint(1, 10)}\" for _ in range(num_rows)],\n",
        "    \"LONGEST_CONSECUTIVE_STREAK\": [random.randint(1, 20) for _ in range(num_rows)],\n",
        "    \"TERMS_ATTENDED\": [random.randint(3, 18) for _ in range(num_rows)],\n",
        "    \"HOURS_ATTENDED\": [random.randint(0, 8000) for _ in range(num_rows)],\n",
        "    \"AGE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "   # \"AGE_CAT_BY_10\": [f\"{age // 10 * 10}s\" for age in data[\"AGE\"]],\n",
        "    \"ANONYMOUS_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"RECORD_STATUS\": [fake.word() for _ in range(num_rows)],\n",
        "    \"ENROLLED_YEAR\": [random.randint(1945, 2020) for _ in range(num_rows)],\n",
        "    \"ENROLLED_SCHOOL\": [f\"School_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"IS_FIRST_GEN_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FIRST_DEGREE_YEAR\": [random.randint(1950, 2020) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_LAST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_FIRST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"FIRST_LAST_DEGREE_DIFF\": [random.uniform(0, 10) for _ in range(num_rows)],\n",
        "    \"Age_at_FIRST_DEGREE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "    \"P_COUNTRY\": [random.choice([\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"Japan\", \"India\", \"Australia\"]) for _ in range(num_rows)],\n",
        "#     \"P_STATE\": [random.choice(us_states) if country == \"USA\" else None for country in data[\"P_COUNTRY\"]],\n",
        "     \"ALUMNUS_DEGREED\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ALUMNUS_NONDEGREED\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXTERNAL_CONTACT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FORMER_EMPLOYEE_ALL\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FORMER_SPECIFIC_EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FRIEND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"HOUSESTAFF\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"SPECIFC_EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"SPECIFIC_FRIEND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ANONYMOUS_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"RECORD_STATUS\": [fake.word() for _ in range(num_rows)],\n",
        "    \"ENROLLED_YEAR\": [random.randint(1945, 2020) for _ in range(num_rows)],\n",
        "    \"ENROLLED_SCHOOL\": [f\"School_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"IS_FIRST_GEN_YN\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"FIRST_DEGREE_YEAR\": [random.randint(1950, 2020) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_LAST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_FIRST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"FIRST_LAST_DEGREE_DIFF\": [random.uniform(0, 10) for _ in range(num_rows)],\n",
        "    \"Age_at_FIRST_DEGREE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "    \"SPOUSE_IS_DECEASED_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MARITAL_STATUS\": [random.choice([\"Single\", \"Married\", \"Divorced\"]) for _ in range(num_rows)],\n",
        "    \"SOLICITABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PHONABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MAILABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EMAILABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"GDPR_HOLD_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOP_MANAGER_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"LAST_SUBST_CONTACT_DATE\": [fake.date_between(start_date=\"-5y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_SUBST_CONTACT_UNIT\": [f\"Unit_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"STAGE_OF_READINESS\": [f\"Stage_{random.randint(1, 9)}\" for _ in range(num_rows)],\n",
        "    \"PROPOSAL_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"WEALTH_RATING\": [random.randint(0, 20) for _ in range(num_rows)],\n",
        "    \"PARENT_OF_CURRENT_STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT_FIRST_TIME_IN_COLLEGE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT_CURRENT_UNDERGRAD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ENROLLED_CHILDREN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COUNT_ENROLLED_CHILDREN\": [random.randint(0, 4) for _ in range(num_rows)],\n",
        "    \"PARENT_HONORS\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"DEGREE_COUNT\": [random.randint(0, 5) for _ in range(num_rows)],\n",
        "    \"P_STATE\": [fake.state_abbr() for _ in range(num_rows)],\n",
        "    \"P_COUNTRY\": [random.choice([\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"Japan\", \"India\", \"Australia\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_COMMITS\": [random.randint(0, 100) for _ in range(num_rows)],\n",
        "    \"LEGACY_SOCIETY\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRESIDENTS_COUNCIL\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRESIDENTIAL_PROSPECTS\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRINCIPAL_GIFTS\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"CELEBRITY\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXEC_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NATIONAL_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"LIFE_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"A_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PHILANTHROPIC_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ENGAGEMENT_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXPERIENTIAL_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COMMUNICATION_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_CREDENTIALS\": [random.randint(0, 7) for _ in range(num_rows)],\n",
        "    \"LAST_ACTIVITY_DATE\": [fake.date_between(start_date=\"-3y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_ACTIVITY_TYPE\": [fake.word() for _ in range(num_rows)],\n",
        "    \"IS_LEGACY_STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"IS_ADV_BOARD_MEMBER\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"IS_ALUM_BOARD_MEMBER\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ATTENDED_EVENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_EVENTS_ATTENDED\": [random.randint(0, 23) for _ in range(num_rows)],\n",
        "    \"FRAT_OR_SOROR\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"HONOR_SOCIETY_IND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NUM_AWARDS_RCVD\": [random.randint(0, 6) for _ in range(num_rows)],\n",
        "    \"NUM_ACTIVE_PLEDGES\": [random.randint(0, 3) for _ in range(num_rows)],\n",
        "    \"GIVING_SOCIETY\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        " #   \"DISTANCE\": [random.uniform(10, 4500) if country == \"USA\" else None for country in data[\"P_COUNTRY\"]],\n",
        "   \"A_MEMB_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"A_MEMB_TYPE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NUM_OPEN_PROPOSALS\": [random.randint(0, 3) for _ in range(num_rows)],\n",
        "    \"MG_MODEL_SCORE\": [random.uniform(0, 5) for _ in range(num_rows)]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "import random\n",
        "df['DISTANCE'] = [random.uniform(10, 4500) if c == \"USA\" else None for c in df['P_COUNTRY']]\n",
        "df['P_STATE'] = [random.choice(us_states) if country == \"USA\" else None for country in df['P_COUNTRY']]\n",
        "df['AGE_CAT_BY_10'] = [f\"{age // 10 * 10}s\" for age in df['AGE']]\n",
        "# Set the index to include new variables\n",
        "df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "\n",
        "# Reset the index and update columns\n",
        "df = df.reset_index()\n",
        "df.columns  # Force recalculation of columns attribute\n",
        "\n",
        "# df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "# Save to CSV\n",
        "# output_path = \"/content/Randomized_Dataset.csv\"\n",
        "# df.to_csv(output_path, index=True)\n",
        "\n",
        "# print(f\"Dataset with {len(data)} fields saved to {output_path}\")\n",
        "# df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "W3hnb15-rG-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for all columns\n",
        "df.shape[1]\n"
      ],
      "metadata": {
        "id": "VVrHvjNDA-ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#create list of columns names\n",
        "for column manipulation\n"
      ],
      "metadata": {
        "id": "NpCiYjrT9Wzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = df.columns\n",
        "\n",
        "# Convert the column names to a list\n",
        "column_list = df.columns.tolist()\n",
        "\n",
        "print(column_names)\n",
        "print(column_list)"
      ],
      "metadata": {
        "id": "JbXntlNz-O_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UNNECESSARY\n",
        "\n",
        "> # Applying changes to the DataFrame\n",
        "df['DISTANCE'] = [random.uniform(10, 4500) if c == \"USA\" else None for c in df['P_COUNTRY']]\n",
        "df['P_STATE'] = [random.choice(us_states) if country == \"USA\" else None for country in df['P_COUNTRY']]\n",
        "df['AGE_CAT_BY_10'] = [f\"{age // 10 * 10}s\" for age in df['AGE']]\n",
        "\n",
        "# Set the index to include 'DISTANCE' and 'P_STATE'\n",
        "> df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "\n",
        "# Reset the index and update columns\n",
        "df = df.reset_index()\n",
        "df.columns  # Force recalculation of columns attribute\n",
        "\n",
        "# Save the updated DataFrame\n",
        "output_path = \"/content/Randomized_Dataset.csv\"\n",
        "df.to_csv(output_path, index=False) # Save without index\n",
        "\n",
        "print(f\"Updated Dataset saved to {output_path}\")\n",
        "print(df.columns) # Verify columns now include 'DISTANCE' and 'P_STATE'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lvRj9IXi_-pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General review of data\n"
      ],
      "metadata": {
        "id": "2RApdVkXIOtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 1. Data Shape\n",
        "print(f\"Data Shape: {df.shape}\")\n",
        "\n",
        "# 2. Data Types\n",
        "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "\n",
        "# 3. Descriptive Statistics\n",
        "print(f\"\\nDescriptive Statistics:\\n{df.describe()}\")\n",
        "\n",
        "# 4. Unique Values\n",
        "# for col in ['bird_name', 'device_info_serial']:\n",
        "#    print(f\"\\nUnique Values for {col}:\\n{df[col].value_counts()}\")\n",
        "\n",
        "# 5. Missing Values\n",
        "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# 6. Correlations\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate correlations on the numeric DataFrame\n",
        "print(f\"\\nCorrelations:\\n{numeric_df.corr()}\")\n"
      ],
      "metadata": {
        "id": "6D-FE7Jgui3-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4): data_wrangling\n",
        "\n",
        "### Subtask:\n",
        "Convert qualitative variables into numerical representations using appropriate encoding techniques\n",
        "\n"
      ],
      "metadata": {
        "id": "QZ_4c9y6FBvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('/content/Randomized_Dataset.csv')\n",
        "\n",
        "\n",
        "# Display data types and check for missing values.\n",
        "print(df.info())\n",
        "\n",
        "# Get summary statistics for numerical columns.\n",
        "print(df.describe())\n",
        "\n",
        "# Identify qualitative variables.\n",
        "qualitative_vars = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Qualitative variables: {qualitative_vars}\")\n",
        "\n",
        "# Explore target variables.\n",
        "target_vars = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "for target in target_vars:\n",
        "    print(f\"\\n--- {target} ---\")\n",
        "    print(df[target].value_counts())\n",
        "    # If numerical, plot a histogram.\n",
        "    if df[target].dtype != 'object':\n",
        "        df[target].hist()\n",
        "        plt.title(f\"Distribution of {target}\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "9idAY0t3E984"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fill in missing data and convert qualitative variables to numberic"
      ],
      "metadata": {
        "id": "8L4nc__op4zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "# Handle missing values (if any) - for simplicity, we'll fill with the mean for numerical and mode for categorical.\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    else:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "# Convert qualitative variables to numerical using Label Encoding.\n",
        "qualitative_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "for col in qualitative_vars:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n"
      ],
      "metadata": {
        "id": "5OYi6_HEK4dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the data in the four DataFrames (`df_major_cash`, `df_major_pledge`, `df_commit_major`, `df_inflation_major_commit`) to prepare them for model training. This involves dropping irrelevant columns and converting qualitative variables into numerical representations. Subsetting the data to preserve main dataset\n"
      ],
      "metadata": {
        "id": "GjAULoarqLS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_major_cash = df.copy()  # Creating a copy to avoid modifying the original DataFrame\n",
        "df_major_pledge = df.copy()\n",
        "df_commit_major = df.copy()\n",
        "df_inflation_major_commit = df.copy()\n",
        "\n",
        "# # Drop irrelevant columns\n",
        "df_major_cash = df_major_cash.drop(columns=['MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_major_pledge = df_major_pledge.drop(columns=['MAJOR_CASH', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_commit_major = df_commit_major.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_inflation_major_commit = df_inflation_major_commit.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR'])\n",
        "#\n",
        "# Convert qualitative variables to numerical representations using one-hot encoding\n",
        "def convert_qualitative_to_numerical(df):\n",
        "    qualitative_vars = df.select_dtypes(include=['object']).columns\n",
        "    df = pd.get_dummies(df, columns=qualitative_vars)\n",
        "    return df\n",
        "\n",
        "df_major_cash = convert_qualitative_to_numerical(df_major_cash)\n",
        "df_major_pledge = convert_qualitative_to_numerical(df_major_pledge)\n",
        "df_commit_major = convert_qualitative_to_numerical(df_commit_major)\n",
        "df_inflation_major_commit = convert_qualitative_to_numerical(df_inflation_major_commit)"
      ],
      "metadata": {
        "id": "rOqS8E6RFk8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MEAT AND POTATOES"
      ],
      "metadata": {
        "id": "HoKL_FYQEUWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RANDOM FOREST"
      ],
      "metadata": {
        "id": "yHaIqnqvDE37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install scikit-learn #Install scikit-learn if not already installed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "target_variables = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "\n",
        "# Dictionary to store the split data for each target variable\n",
        "split_data_dict = {}\n",
        "\n",
        "# Loop through target variables and split data\n",
        "for target_variable in target_variables:\n",
        "    X = df.drop(columns=[target_variable])  # Features\n",
        "    y = df[target_variable]  # Target\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Store the split data in the dictionary\n",
        "\n",
        "    split_data_dict[target_variable] = {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test\n",
        "    }\n",
        "\n",
        "# Loop through split_data_dict and create variables with suffixes\n",
        "for target_variable, data_splits in split_data_dict.items():\n",
        "    for data_type, data in data_splits.items():\n",
        "        variable_name = f\"{data_type}_{target_variable}\"  # Create variable name with suffix\n",
        "        exec(f\"{variable_name} = data\")  # Assign data to the variable\n",
        "    # Now you can use X_train, X_test, y_train, y_test for model training and evaluation\n",
        "    # for the current target_variable\n",
        "    print(f\"Training model for {target_variable}...{data_type}_{target_variable}\")\n",
        "\n",
        "# Accessing the split data for a specific target variable\n",
        "# Example:\n",
        "# X_train_major_cash = split_data_dict['MAJOR_CASH']['X_train']\n",
        "# y_train_major_cash = split_data_dict['MAJOR_CASH']['y_train']\n",
        "# ... and so on for other target variables and data splits"
      ],
      "metadata": {
        "id": "jyvdpPGGvUJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "target_variables = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "\n",
        "# Dictionary to store the split data for each target variable\n",
        "split_data_dict = {}\n",
        "\n",
        "# Loop through target variables and split data\n",
        "for target_variable in target_variables:\n",
        "    X = df.drop(columns=[target_variable])  # Features\n",
        "    y = df[target_variable]  # Target\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Store the split data in the dictionary\n",
        "\n",
        "    split_data_dict[target_variable] = {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test\n",
        "    }\n",
        "\n",
        "# Create and train RandomForest models for each DataFrame\n",
        "models = {}  # Dictionary to store the models\n",
        "\n",
        "# Loop through split_data_dict and train models\n",
        "for target_variable, data_splits in split_data_dict.items():\n",
        "    # Access the split data\n",
        "    X_train = data_splits['X_train']\n",
        "    X_test = data_splits['X_test']\n",
        "    y_train = data_splits['y_train']\n",
        "    y_test = data_splits['y_test']\n",
        "\n",
        "\n",
        "    # Create and train the model\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Store the model in the dictionary\n",
        "    models[target_variable] = model\n",
        "\n",
        "    print(f\"Training model for {target_variable}...\")\n",
        "\n",
        "print(\"RandomForest models trained and stored in the 'models' dictionary.\")"
      ],
      "metadata": {
        "id": "-Uvj_4h1B14q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install scikit-learn #Install scikit-learn if not already installed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assuming 'target_variable' is the name of your target variable column\n",
        "target_variable = 'MAJOR_CASH'  # Replace 'your_target_variable' with the actual name\n",
        "\n",
        "# Function to split data into training and testing sets\n",
        "def split_data(df, target_variable):\n",
        "    X = df.drop(columns=[target_variable])\n",
        "    y = df[target_variable]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size as needed\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Split data for each DataFrame\n",
        "X_train_major_cash, X_test_major_cash, y_train_major_cash, y_test_major_cash = split_data(df_major_cash, target_variable)\n",
        "X_train_major_pledge, X_test_major_pledge, y_train_major_pledge, y_test_major_pledge = split_data(df_major_pledge, target_variable)\n",
        "X_train_commit_major, X_test_commit_major, y_train_commit_major, y_test_commit_major = split_data(df_commit_major, target_variable)\n",
        "X_train_INFLATION_MAJOR_COMMIT, X_test_INFLATION_MAJOR_COMMIT, y_train_INFLATION_MAJOR_COMMIT, y_test_INFLATION_MAJOR_COMMIT = split_data(df_INFLATION_MAJOR_COMMIT, target_variable)\n",
        "\n",
        "# ... (Rest of your code for model training)"
      ],
      "metadata": {
        "id": "o-3lx6d-suMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target_variables = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "\n",
        "# Dictionary to store the split data for each target variable\n",
        "split_data_dict = {}\n",
        "\n",
        "# Loop through target variables and split data\n",
        "for target_variable in target_variables:\n",
        "    X = df.drop(columns=[target_variable])  # Features\n",
        "    y = df[target_variable]  # Target\n",
        "    \n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Store the split data in the dictionary\n",
        "\t\t\t\t\t\t\t\t\t\t  \n",
        "    split_data_dict[target_variable] = {\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\t  \n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test\n",
        "    }\n",
        "\n",
        "# Accessing the split data for a specific target variable\n",
        "# Example:\n",
        "X_train_major_cash = split_data_dict['MAJOR_CASH']['X_train']\n",
        "y_train_major_cash = split_data_dict['MAJOR_CASH']['y_train']\n",
        "# ... and so on for other target variables and data splits"
      ],
      "metadata": {
        "id": "WrQgAIo0uhu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create and train RandomForest models for each DataFrame\n",
        "models = {}  # Dictionary to store the models\n",
        "\n",
        "model_major_cash = RandomForestClassifier(random_state=42)\n",
        "model_major_cash.fit(X_train_major_cash, y_train_major_cash)\n",
        "models['MAJOR_CASH'] = model_major_cash\n",
        "\n",
        "# Model for df_major_pledge\n",
        "model_major_pledge = RandomForestClassifier(random_state=42)\n",
        "model_major_pledge.fit(X_train, y_train)\n",
        "models['MAJOR_PLEDGE'] = model_major_pledge\n",
        "\n",
        "# Model for df_commit_major\n",
        "model_commit_major = RandomForestClassifier(random_state=42)\n",
        "model_commit_major.fit(X_train, y_train)\n",
        "models['COMMIT_MAJOR'] = model_commit_major\n",
        "\n",
        "# Model for df_INFLATION_MAJOR_COMMIT\n",
        "model_INFLATION_MAJOR_COMMIT = RandomForestClassifier(random_state=42)\n",
        "model_INFLATION_MAJOR_COMMIT.fit(X_train, y_train)\n",
        "models['INFLATION_MAJOR_COMMIT'] = model_INFLATION_MAJOR_COMMIT\n",
        "\n",
        "print(\"RandomForest models trained and stored in the 'models' dictionary.\")"
      ],
      "metadata": {
        "id": "3eil9K4A0bEy"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "target_variables = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "\n",
        "# Dictionary to store the split data and model results for each target variable\n",
        "split_data_dict = {}\n",
        "\n",
        "# Loop through target variables and split data\n",
        "for target_variable in target_variables:\n",
        "    X = df.drop(columns=[target_variable])  # Features\n",
        "    y = df[target_variable]  # Target\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Store the split data in the dictionary\n",
        "\n",
        "    split_data_dict[target_variable] = {\n",
        "        'X_train': X_train,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'model': None,  # Initialize model to None\n",
        "        'accuracy': None  # Initialize accuracy to None\n",
        "    }\n",
        "\n",
        "# Create and train RandomForest models for each DataFrame and store results\n",
        "\n",
        "# Loop through split_data_dict and train models\n",
        "for target_variable, data_splits in split_data_dict.items():\n",
        "    # Access the split data\n",
        "    X_train = data_splits['X_train']\n",
        "    X_test = data_splits['X_test']\n",
        "    y_train = data_splits['y_train']\n",
        "    y_test = data_splits['y_test']\n",
        "\n",
        "\n",
        "    # Create and train the model\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Store the model in the dictionary\n",
        "    split_data_dict[target_variable]['model'] = model\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the accuracy in the dictionary\n",
        "    split_data_dict[target_variable]['accuracy'] = accuracy\n",
        "\n",
        "    # Print the accuracy\n",
        "    print(f\"Accuracy for {target_variable}: {accuracy}\")\n",
        "\n",
        "print(\"RandomForest models trained, evaluated, and results stored in the 'split_data_dict' dictionary.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LDVsCP8VDx9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION, MEH"
      ],
      "metadata": {
        "id": "_qqevskDC1hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LOGISTIC REGRESSION\n"
      ],
      "metadata": {
        "id": "NeRxbZn6LuQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FH9gRYjeJuW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare data for model training.\n",
        "target_vars = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "X = df.drop(columns=target_vars)\n",
        "y = df[target_vars]\n",
        "\n",
        "# Split data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale numerical features using StandardScaler.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train and evaluate models for each target variable.\n",
        "models = {}\n",
        "for target in target_vars:\n",
        "    model = LogisticRegression(random_state=42)\n",
        "    model.fit(X_train, y_train[target])\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test[target], y_pred)\n",
        "    models[target] = model\n",
        "    print(f\"Model for {target}: Accuracy = {accuracy}\")\n",
        "\n",
        "df_major_cash = df.copy()\n",
        "df_major_pledge = df.copy()\n",
        "df_commit_major = df.copy()\n",
        "df_inflation_major_commit = df.copy()\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# # Drop irrelevant columns\n",
        "# df_major_cash = df_major_cash.drop(columns=['MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "# df_major_pledge = df_major_pledge.drop(columns=['MAJOR_CASH', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "# df_commit_major = df_commit_major.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'INFLATION_MAJOR_COMMIT'])\n",
        "# df_inflation_major_commit = df_inflation_major_commit.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR'])\n",
        "#\n",
        "# Convert qualitative variables to numerical representations using one-hot encoding\n",
        "def convert_qualitative_to_numerical(df):\n",
        "    qualitative_vars = df.select_dtypes(include=['object']).columns\n",
        "    df = pd.get_dummies(df, columns=qualitative_vars)\n",
        "    return df\n",
        "\n",
        "df_major_cash = convert_qualitative_to_numerical(df_major_cash)\n",
        "df_major_pledge = convert_qualitative_to_numerical(df_major_pledge)\n",
        "df_commit_major = convert_qualitative_to_numerical(df_commit_major)\n",
        "df_inflation_major_commit = convert_qualitative_to_numerical(df_inflation_major_commit)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "for df in [df_major_cash, df_major_pledge, df_commit_major, df_inflation_major_commit]:\n",
        "    for col in df.select_dtypes(include=['number']).columns:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(df[col].mean())\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def select_and_impute(df, target_variable):\n",
        "    \"\"\"Selects relevant features and handles missing values.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame to process.\n",
        "        target_variable: The name of the target variable column.\n",
        "\n",
        "    Returns:\n",
        "        The processed DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Feature selection based on potential relevance to target variables\n",
        "    relevant_features = [\n",
        "        'TOTAL_COMMIT_VALUE', 'BASELINE', 'PRINCIPAL_GIFT', 'BELOW_BASELINE',\n",
        "        'LEADERSHIP_GIFT', 'BASELINE_NO_LEADERSHIP', 'LEGAL_CREDIT', 'CASH',\n",
        "        'PLEDGE', 'DEFERRED', 'CASH_RECEIVED', 'OUTSTANDING_BALANCE',\n",
        "        'NON_GIFT', 'FIRST_GIFT_AMOUNT', 'INFLATION_ADJUSTED_FIRST_AMOUNT',\n",
        "        'LAST_AMOUNT', 'INFLATION_ADJ_LAST_AMOUNT', 'LARGEST_AMOUNT',\n",
        "        'INFLATION_ADJ_LARGEST_AMOUNT', 'PRIMARY_UNIT_LIFETIME_FUNDRAISING',\n",
        "        'TOTAL_YEARS_GIVING', 'RECENT_CONSECUTIVE_STREAK_GIVING',\n",
        "        'LONGEST_CONSECUTIVE_STREAK', 'TERMS_ATTENDED', 'HOURS_ATTENDED',\n",
        "        'AGE', 'AGE_CAT_BY_10', 'ANONYMOUS_YN', 'IS_FIRST_GEN_YN',\n",
        "        'YEARS_SINCE_LAST_DEGREE', 'YEARS_SINCE_FIRST_DEGREE',\n",
        "        'FIRST_LAST_DEGREE_DIFF', 'Age_at_FIRST_DEGREE', 'DISTANCE',\n",
        "        'ALUMNUS_DEGREED', 'ALUMNUS_NONDEGREED', 'EXTERNAL_CONTACT',\n",
        "        'FORMER_EMPLOYEE_ALL', 'FORMER_SPECIFIC_EMPLOYEE', 'FRIEND',\n",
        "        'HOUSESTAFF', 'PARENT', 'STUDENT', 'EMPLOYEE', 'SPECIFC_EMPLOYEE',\n",
        "        'SPECIFIC_FRIEND', 'SPOUSE_IS_DECEASED_YN', 'SOLICITABLE_YN',\n",
        "        'PHONABLE_YN', 'MAILABLE_YN', 'EMAILABLE_YN', 'GDPR_HOLD_YN',\n",
        "        'TOP_MANAGER_YN', 'STAGE_OF_READINESS', 'PROPOSAL_YN',\n",
        "        'WEALTH_RATING', 'PARENT_OF_CURRENT_STUDENT',\n",
        "        'PARENT_FIRST_TIME_IN_COLLEGE', 'PARENT_CURRENT_UNDERGRAD',\n",
        "        'ENROLLED_CHILDREN', 'COUNT_ENROLLED_CHILDREN', 'PARENT_HONORS',\n",
        "        'DEGREE_COUNT', 'TOTAL_COMMITS', 'LEGACY_SOCIETY',\n",
        "        'PRESIDENTS_COUNCIL', 'PRESIDENTIAL_PROSPECTS', 'PRINCIPAL_GIFTS',\n",
        "        'CELEBRITY', 'EXEC_BOARD', 'NATIONAL_BOARD', 'LIFE_BOARD', 'A_BOARD',\n",
        "        'PHILANTHROPIC_FLAG', 'ENGAGEMENT_FLAG', 'EXPERIENTIAL_FLAG',\n",
        "        'COMMUNICATION_FLAG', 'TOTAL_CREDENTIALS', 'IS_LEGACY_STUDENT',\n",
        "        'IS_ADV_BOARD_MEMBER', 'IS_ALUM_BOARD_MEMBER', 'ATTENDED_EVENT',\n",
        "        'TOTAL_EVENTS_ATTENDED', 'FRAT_OR_SOROR', 'HONOR_SOCIETY_IND',\n",
        "        'NUM_AWARDS_RCVD', 'NUM_ACTIVE_PLEDGES', 'GIVING_SOCIETY',\n",
        "        'A_MEMB_YN', 'A_MEMB_TYPE', 'NUM_OPEN_PROPOSALS', 'MG_MODEL_SCORE'\n",
        "    ]\n",
        "\n",
        "    # Select relevant features and target variable\n",
        "    df = df[[target_variable] + relevant_features]\n",
        "\n",
        "    # Handle missing values\n",
        "    for column in df.columns:\n",
        "        if df[column].isnull().any():\n",
        "            if df[column].dtype == 'object':  # Categorical feature\n",
        "                df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "            else:  # Numerical feature\n",
        "                df[column].fillna(df[column].mean(), inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def one_hot_encode_categorical(df):\n",
        "    \"\"\"One-hot encodes categorical features in the DataFrame.\"\"\"\n",
        "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    if categorical_features:  # Check if there are any categorical features\n",
        "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # sparse=False for DataFrame output\n",
        "        encoded_data = encoder.fit_transform(df[categorical_features])\n",
        "        encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features))\n",
        "        df = df.drop(columns=categorical_features)  # Drop original categorical features\n",
        "        df = pd.concat([df, encoded_df], axis=1)  # Concatenate encoded features\n",
        "    return df\n",
        "\n",
        "# Apply one-hot encoding\n",
        "df_major_cash, df_major_pledge, df_commit_major, df_inflation_major_commit = [one_hot_encode_categorical(df) for df in [df_major_cash, df_major_pledge, df_commit_major, df_inflation_major_commit]]\n",
        "\n",
        "# Apply the function to each DataFrame\n",
        "df_major_cash = select_and_impute(df_major_cash, 'MAJOR_CASH')\n",
        "df_major_pledge = select_and_impute(df_major_pledge, 'MAJOR_PLEDGE')\n",
        "df_commit_major = select_and_impute(df_commit_major, 'COMMIT_MAJOR')\n",
        "df_inflation_major_commit= select_and_impute(df_inflation_major_commit, 'INFLATION_MAJOR_COMMIT')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split df_major_cash\n",
        "X_major_cash = df_major_cash.drop('MAJOR_CASH', axis=1)\n",
        "y_major_cash = df_major_cash['MAJOR_CASH']\n",
        "X_train_major_cash, X_test_major_cash, y_train_major_cash, y_test_major_cash = train_test_split(\n",
        "    X_major_cash, y_major_cash, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Split df_major_pledge\n",
        "X_major_pledge = df_major_pledge.drop('MAJOR_PLEDGE', axis=1)\n",
        "y_major_pledge = df_major_pledge['MAJOR_PLEDGE']\n",
        "X_train_major_pledge, X_test_major_pledge, y_train_major_pledge, y_test_major_pledge = train_test_split(\n",
        "    X_major_pledge, y_major_pledge, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Split df_commit_major\n",
        "X_commit_major = df_commit_major.drop('COMMIT_MAJOR', axis=1)\n",
        "y_commit_major = df_commit_major['COMMIT_MAJOR']\n",
        "X_train_commit_major, X_test_commit_major, y_train_commit_major, y_test_commit_major = train_test_split(\n",
        "    X_commit_major, y_commit_major, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Split INFLATION_MAJOR_COMMIT\n",
        "X_inflation_major_commit = df_inflation_major_commit.drop('INFLATION_MAJOR_COMMIT', axis=1)\n",
        "y_inflation_major_commit = df_inflation_major_commit['INFLATION_MAJOR_COMMIT']\n",
        "X_train_inflation_major_commit, X_test_inflation_major_commit, y_train_inflation_major_commit, y_test_inflation_major_commit = train_test_split(\n",
        "    X_inflation_major_commit, y_inflation_major_commit, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create and train models for each DataFrame\n",
        "models = {}  # Dictionary to store the models\n",
        "\n",
        "# Model for df_major_cash\n",
        "model_major_cash = LogisticRegression(random_state=42, solver='saga', max_iter=10000)\n",
        "model_major_cash.fit(X_train_major_cash, y_train_major_cash)\n",
        "models['MAJOR_CASH'] = model_major_cash\n",
        "\n",
        "# Model for df_major_pledge\n",
        "model_major_pledge = LogisticRegression(random_state=42, solver='saga', max_iter=100000)\n",
        "model_major_pledge.fit(X_train_major_pledge, y_train_major_pledge)\n",
        "models['MAJOR_PLEDGE'] = model_major_pledge\n",
        "\n",
        "# Model for df_commit_major\n",
        "model_commit_major = LogisticRegression(random_state=42, solver='saga', max_iter=1000)\n",
        "model_commit_major.fit(X_train_commit_major, y_train_commit_major)\n",
        "models['COMMIT_MAJOR'] = model_commit_major\n",
        "\n",
        "# Model for df_inflation_major_commit\n",
        "model_inflation_major_commit = LogisticRegression(random_state=42, solver='saga', max_iter=1000)\n",
        "model_inflation_major_commit.fit(X_train_inflation_major_commit, y_train_inflation_major_commit)\n",
        "models['INFLATION_MAJOR_COMMIT'] = model_inflation_major_commit\n",
        "\n",
        "print(\"Models trained and stored in the 'models' dictionary.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UtnotMainaP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate the model for df_major_cash\n",
        "y_pred_major_cash = models['MAJOR_CASH'].predict(X_test_major_cash)  # Assuming 'models' is defined earlier\n",
        "accuracy_major_cash = accuracy_score(y_test_major_cash, y_pred_major_cash)\n",
        "print(f\"Accuracy for MAJOR_CASH: {accuracy_major_cash}\")\n",
        "\n",
        "# Evaluate the model for df_major_pledge\n",
        "y_pred_major_pledge = models['MAJOR_PLEDGE'].predict(X_test_major_pledge)\n",
        "accuracy_major_pledge = accuracy_score(y_test_major_pledge, y_pred_major_pledge)\n",
        "print(f\"Accuracy for MAJOR_PLEDGE: {accuracy_major_pledge}\")\n",
        "\n",
        "# Evaluate the model for df_commit_major\n",
        "y_pred_commit_major = models['COMMIT_MAJOR'].predict(X_test_commit_major)\n",
        "accuracy_commit_major = accuracy_score(y_test_commit_major, y_pred_commit_major)\n",
        "print(f\"Accuracy for COMMIT_MAJOR: {accuracy_commit_major}\")\n",
        "\n",
        "# Evaluate the model for df_inflation_major_commit\n",
        "y_pred_inflation_major_commit = models['INFLATION_MAJOR_COMMIT'].predict(X_test_inflation_major_commit)\n",
        "accuracy_inflation_major_commit = accuracy_score(y_test_inflation_major_commit, y_pred_inflation_major_commit)\n",
        "print(f\"Accuracy for INFLATION_MAJOR_COMMIT: {accuracy_inflation_major_commit}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U6p1ti1AgCFA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}