{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyoungg/Colab_stuff/blob/main/random_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Predict each of these variables independently MAJOR_CASH, MAJOR_PLEDGE, COMMIT_MAJOR, INFLATION_MAJOR_COMMIT, some of the data may also require conversion from qualitative variables. By independent I mean I would like to not consider MAJOR_CASH, MAJOR_PLEDGE, COMMIT_MAJOR, INFLATION_MAJOR_COMMIT as part of the entire dataset\n",
        "\n",
        "Here is all the data you need:\n",
        "/tmp/Complete_Randomized_Dataset.csv"
      ],
      "metadata": {
        "id": "1WZRP7JL7_Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) data_loading\n",
        "\n",
        "### Subtask:\n",
        "Load the data from the provided CSV file into a pandas DataFrame."
      ],
      "metadata": {
        "id": "qjKFexPT8WKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan\n",
        "\n",
        "1. **data_loading**: Load the data SQL into a pandas DataFrame using `pd.read_csv()`.\n",
        "2. **data_exploration**:\n",
        "    - Use `df.info()` and `df.describe()` to understand the data types and distributions of each variable, including `MAJOR_CASH`, `MAJOR_PLEDGE`, `COMMIT_MAJOR`, and `INFLATION_MAJOR_COMMIT`.\n",
        "    - Identify qualitative variables that need conversion by checking their data types.\n",
        "3. **data_preparation**: Create four separate copies of the DataFrame using `df.copy()` for predicting each target variable (`MAJOR_CASH`, `MAJOR_PLEDGE`, `COMMIT_MAJOR`, `INFLATION_MAJOR_COMMIT`).\n",
        "4. **data_wrangling**:\n",
        "    - In each DataFrame copy, drop the other three target variables using `df.drop(columns=['column_name'])`.\n",
        "    - Convert qualitative variables into numerical representations using appropriate encoding techniques (e.g., `pd.get_dummies()` for one-hot encoding) within each DataFrame copy.\n",
        "5. **feature_engineering**:\n",
        "    - For each DataFrame copy, select relevant features for prediction. This may involve dropping irrelevant columns using `df.drop(columns=['column_name'])` or creating new features based on existing ones using Python 3 syntax.\n",
        "    - Handle missing values in the features using appropriate techniques (e.g., `df.fillna()` for imputation) within each DataFrame copy.\n",
        "6. **data_splitting**: For each DataFrame copy, split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`.\n",
        "7. **model_training**: Train a separate machine learning model (e.g., regression, classification) for each target variable using its corresponding DataFrame copy. Choose the model based on the nature of the target variable (continuous or categorical). Import necessary models from `sklearn` using Python 3 syntax.\n",
        "    - **model_training**: Train a model to predict `MAJOR_CASH` using the first DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `MAJOR_PLEDGE` using the second DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `COMMIT_MAJOR` using the third DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `INFLATION_MAJOR_COMMIT` using the fourth DataFrame copy.\n",
        "8. **model_evaluation**: Evaluate the performance of each model on its corresponding testing set using appropriate metrics (e.g., R-squared, accuracy, precision, recall). Import necessary metrics from `sklearn.metrics` using Python 3 syntax.\n",
        "    - **model_evaluation**: Evaluate the `MAJOR_CASH` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `MAJOR_PLEDGE` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `COMMIT_MAJOR` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `INFLATION_MAJOR_COMMIT` prediction model.\n",
        "9. **finish_task**: Write a summary report describing the process, the models trained for each target variable, their performance, and any insights gained from the analysis. Include recommendations for future work."
      ],
      "metadata": {
        "id": "_fuZlgQw8NOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install necessary packages"
      ],
      "metadata": {
        "id": "x2OOy6mM8t00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker\n",
        "!pip install scipy\n",
        "!pip install scikit-learn #Install scikit-learn if not already installed"
      ],
      "metadata": {
        "id": "Wq6x0jf8rfnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Random test data"
      ],
      "metadata": {
        "id": "CG_QFfYI9Fft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addtion creating correlations of:\n",
        "FRIEND being 65 percent correlated to major gift = yes\n",
        "ATTENDED_EVENT being 25 perscent negatively correlated with major gift = yes\n",
        "Distance being 75 percent positively correlated with major gift = yes"
      ],
      "metadata": {
        "id": "edr4c6dX0ZRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Initialize Faker and constants\n",
        "fake = Faker()\n",
        "num_rows = 10000\n",
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\",\n",
        "    \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\",\n",
        "    \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\",\n",
        "    \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n",
        "    \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "# Define base probabilities and relationships\n",
        "major_cash_probs = norm.cdf(np.random.randn(num_rows))  # N(0, 1), transformed to probabilities\n",
        "major_cash = np.where(major_cash_probs > 0.5, \"yes\", \"no\")\n",
        "\n",
        "# FRIEND correlated with MAJOR_CASH (65%)\n",
        "friend_probs = norm.cdf(np.random.randn(num_rows) + 0.65 * (major_cash == \"yes\"))\n",
        "friend = np.where(friend_probs > 0.5, \"yes\", \"no\")\n",
        "\n",
        "# ATTENDED_EVENT negatively correlated with MAJOR_CASH (25%)\n",
        "attended_event_probs = norm.cdf(np.random.randn(num_rows) - 0.25 * (major_cash == \"yes\"))\n",
        "attended_event = np.where(attended_event_probs > 0.5, \"yes\", \"no\")\n",
        "\n",
        "# DISTANCE positively correlated with MAJOR_CASH (75%)\n",
        "distance_mean = np.where(major_cash == \"yes\", 3000, 1000)  # Higher mean when MAJOR_CASH is \"yes\"\n",
        "distance_std = 1000  # Standard deviation\n",
        "distance = np.clip(norm.rvs(loc=distance_mean, scale=distance_std), 10, 4500)  # Clip range\n",
        "\n",
        "\n",
        "\n",
        "# Generate random data for all fields\n",
        "data = {\n",
        "    \"MAJOR_CASH\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MAJOR_PLEDGE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COMMIT_MAJOR\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"INFLATION_MAJOR_COMMIT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_COMMIT_VALUE\": [random.uniform(1000, 100000) for _ in range(num_rows)],\n",
        "    \"BASELINE\": [random.uniform(500, 50000) for _ in range(num_rows)],\n",
        "    \"PRINCIPAL_GIFT\": [random.uniform(100, 10000) for _ in range(num_rows)],\n",
        "    \"BELOW_BASELINE\": [random.uniform(0, 5000) for _ in range(num_rows)],\n",
        "    \"LEADERSHIP_GIFT\": [random.uniform(5000, 50000) for _ in range(num_rows)],\n",
        "    \"BASELINE_NO_LEADERSHIP\": [random.uniform(500, 5000) for _ in range(num_rows)],\n",
        "    \"LEGAL_CREDIT\": [random.uniform(100, 100000) for _ in range(num_rows)],\n",
        "    \"CASH\": [random.uniform(0, 50000) for _ in range(num_rows)],\n",
        "    \"PLEDGE\": [random.uniform(0, 30000) for _ in range(num_rows)],\n",
        "    \"DEFERRED\": [random.uniform(0, 10000) for _ in range(num_rows)],\n",
        "    \"CASH_RECEIVED\": [random.uniform(0, 50000) for _ in range(num_rows)],\n",
        "    \"OUTSTANDING_BALANCE\": [random.uniform(0, 20000) for _ in range(num_rows)],\n",
        "    \"NON_GIFT\": [random.uniform(0, 10000) for _ in range(num_rows)],\n",
        "    \"DATE_FIRST_GIFT\": [fake.date_between(start_date=\"-30y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"FIRST_GIFT_AMOUNT\": [random.uniform(50, 10000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJUSTED_FIRST_AMOUNT\": [random.uniform(50, 15000) for _ in range(num_rows)],\n",
        "    \"DATE_LAST_GIFT\": [fake.date_between(start_date=\"-5y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_AMOUNT\": [random.uniform(50, 10000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJ_LAST_AMOUNT\": [random.uniform(50, 15000) for _ in range(num_rows)],\n",
        "    \"LARGEST_DATE\": [fake.date_between(start_date=\"-10y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LARGEST_AMOUNT\": [random.uniform(100, 20000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJ_LARGEST_AMOUNT\": [random.uniform(100, 25000) for _ in range(num_rows)],\n",
        "    \"PRIMARY_UNIT\": [f\"Unit_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"PRIMARY_UNIT_LIFETIME_FUNDRAISING\": [random.uniform(10000, 500000) for _ in range(num_rows)],\n",
        "    \"TOTAL_YEARS_GIVING\": [random.randint(1, 40) for _ in range(num_rows)],\n",
        "    \"RECENT_CONSECUTIVE_STREAK_GIVING\": [random.randint(1, 10) for _ in range(num_rows)],\n",
        "    \"SEGMENT\": [f\"Segment_{random.randint(1, 10)}\" for _ in range(num_rows)],\n",
        "    \"LONGEST_CONSECUTIVE_STREAK\": [random.randint(1, 20) for _ in range(num_rows)],\n",
        "    \"TERMS_ATTENDED\": [random.randint(3, 18) for _ in range(num_rows)],\n",
        "    \"HOURS_ATTENDED\": [random.randint(0, 8000) for _ in range(num_rows)],\n",
        "    \"AGE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "   # \"AGE_CAT_BY_10\": [f\"{age // 10 * 10}s\" for age in data[\"AGE\"]],\n",
        "    \"ANONYMOUS_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"RECORD_STATUS\": [fake.word() for _ in range(num_rows)],\n",
        "    \"ENROLLED_YEAR\": [random.randint(1945, 2020) for _ in range(num_rows)],\n",
        "    \"ENROLLED_SCHOOL\": [f\"School_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"IS_FIRST_GEN_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FIRST_DEGREE_YEAR\": [random.randint(1950, 2020) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_LAST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_FIRST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"FIRST_LAST_DEGREE_DIFF\": [random.uniform(0, 10) for _ in range(num_rows)],\n",
        "    \"Age_at_FIRST_DEGREE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "    \"P_COUNTRY\": [random.choice([\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"Japan\", \"India\", \"Australia\"]) for _ in range(num_rows)],\n",
        "#     \"P_STATE\": [random.choice(us_states) if country == \"USA\" else None for country in data[\"P_COUNTRY\"]],\n",
        "     \"ALUMNUS_DEGREED\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ALUMNUS_NONDEGREED\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXTERNAL_CONTACT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FORMER_EMPLOYEE_ALL\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FORMER_SPECIFIC_EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FRIEND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"HOUSESTAFF\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"SPECIFC_EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"SPECIFIC_FRIEND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ANONYMOUS_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"RECORD_STATUS\": [fake.word() for _ in range(num_rows)],\n",
        "    \"ENROLLED_YEAR\": [random.randint(1945, 2020) for _ in range(num_rows)],\n",
        "    \"ENROLLED_SCHOOL\": [f\"School_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"IS_FIRST_GEN_YN\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"FIRST_DEGREE_YEAR\": [random.randint(1950, 2020) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_LAST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_FIRST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"FIRST_LAST_DEGREE_DIFF\": [random.uniform(0, 10) for _ in range(num_rows)],\n",
        "    \"Age_at_FIRST_DEGREE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "    \"SPOUSE_IS_DECEASED_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MARITAL_STATUS\": [random.choice([\"Single\", \"Married\", \"Divorced\"]) for _ in range(num_rows)],\n",
        "    \"SOLICITABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PHONABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MAILABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EMAILABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"GDPR_HOLD_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOP_MANAGER_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"LAST_SUBST_CONTACT_DATE\": [fake.date_between(start_date=\"-5y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_SUBST_CONTACT_UNIT\": [f\"Unit_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"STAGE_OF_READINESS\": [f\"Stage_{random.randint(1, 9)}\" for _ in range(num_rows)],\n",
        "    \"PROPOSAL_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"WEALTH_RATING\": [random.randint(0, 20) for _ in range(num_rows)],\n",
        "    \"PARENT_OF_CURRENT_STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT_FIRST_TIME_IN_COLLEGE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT_CURRENT_UNDERGRAD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ENROLLED_CHILDREN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COUNT_ENROLLED_CHILDREN\": [random.randint(0, 4) for _ in range(num_rows)],\n",
        "    \"PARENT_HONORS\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"DEGREE_COUNT\": [random.randint(0, 5) for _ in range(num_rows)],\n",
        "    \"P_STATE\": [fake.state_abbr() for _ in range(num_rows)],\n",
        "    \"P_COUNTRY\": [random.choice([\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"Japan\", \"India\", \"Australia\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_COMMITS\": [random.randint(0, 100) for _ in range(num_rows)],\n",
        "    \"LEGACY_SOCIETY\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRESIDENTS_COUNCIL\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRESIDENTIAL_PROSPECTS\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRINCIPAL_GIFTS\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"CELEBRITY\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXEC_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NATIONAL_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"LIFE_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"A_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PHILANTHROPIC_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ENGAGEMENT_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXPERIENTIAL_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COMMUNICATION_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_CREDENTIALS\": [random.randint(0, 7) for _ in range(num_rows)],\n",
        "    \"LAST_ACTIVITY_DATE\": [fake.date_between(start_date=\"-3y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_ACTIVITY_TYPE\": [fake.word() for _ in range(num_rows)],\n",
        "    \"IS_LEGACY_STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"IS_ADV_BOARD_MEMBER\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"IS_ALUM_BOARD_MEMBER\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ATTENDED_EVENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_EVENTS_ATTENDED\": [random.randint(0, 23) for _ in range(num_rows)],\n",
        "    \"FRAT_OR_SOROR\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"HONOR_SOCIETY_IND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NUM_AWARDS_RCVD\": [random.randint(0, 6) for _ in range(num_rows)],\n",
        "    \"NUM_ACTIVE_PLEDGES\": [random.randint(0, 3) for _ in range(num_rows)],\n",
        "    \"GIVING_SOCIETY\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        " #   \"DISTANCE\": [random.uniform(10, 4500) if country == \"USA\" else None for country in data[\"P_COUNTRY\"]],\n",
        "   \"A_MEMB_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"A_MEMB_TYPE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NUM_OPEN_PROPOSALS\": [random.randint(0, 3) for _ in range(num_rows)],\n",
        "    \"MG_MODEL_SCORE\": [random.uniform(0, 5) for _ in range(num_rows)]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "import random\n",
        "df['DISTANCE'] = [random.uniform(10, 4500) if c == \"USA\" else None for c in df['P_COUNTRY']]\n",
        "df['P_STATE'] = [random.choice(us_states) if country == \"USA\" else None for country in df['P_COUNTRY']]\n",
        "df['AGE_CAT_BY_10'] = [f\"{age // 10 * 10}s\" for age in df['AGE']]\n",
        "# Set the index to include new variables\n",
        "# df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "df = df.reset_index(drop=True)\n",
        "df =  pd.concat([df, pd.DataFrame({\n",
        "     \"MAJOR_CASH_C\": major_cash,\n",
        "    \"FRIEND_C\": friend,\n",
        "    \"ATTENDED_EVENT_C\": attended_event,\n",
        "    \"DISTANCE_C\": distance\n",
        "})], axis=1)\n",
        "\n",
        "\n",
        "# Reset the index and update columns\n",
        "df = df.reset_index()\n",
        "df.columns  # Force recalculation of columns attribute\n",
        "\n",
        "# df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "# Save to CSV\n",
        "# output_path = \"/content/Randomized_Dataset.csv\"\n",
        "# df.to_csv(output_path, index=True)\n",
        "\n",
        "# print(f\"Dataset with {len(data)} fields saved to {output_path}\")\n",
        "# df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "W3hnb15-rG-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for all columns\n",
        "df.shape[1]\n"
      ],
      "metadata": {
        "id": "VVrHvjNDA-ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#create list of columns names\n",
        "for column manipulation\n"
      ],
      "metadata": {
        "id": "NpCiYjrT9Wzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = df.columns\n",
        "\n",
        "# Convert the column names to a list\n",
        "column_list = df.columns.tolist()\n",
        "\n",
        "print(column_names)\n",
        "print(column_list)"
      ],
      "metadata": {
        "id": "JbXntlNz-O_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General review of data\n"
      ],
      "metadata": {
        "id": "2RApdVkXIOtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 1. Data Shape\n",
        "print(f\"Data Shape: {df.shape}\")\n",
        "\n",
        "# 2. Data Types\n",
        "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "\n",
        "# 3. Descriptive Statistics\n",
        "print(f\"\\nDescriptive Statistics:\\n{df.describe()}\")\n",
        "\n",
        "# 4. Unique Values\n",
        "# for col in ['bird_name', 'device_info_serial']:\n",
        "#    print(f\"\\nUnique Values for {col}:\\n{df[col].value_counts()}\")\n",
        "\n",
        "# 5. Missing Values\n",
        "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# 6. Correlations\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate correlations on the numeric DataFrame\n",
        "print(f\"\\nCorrelations:\\n{numeric_df.corr()}\")\n"
      ],
      "metadata": {
        "id": "6D-FE7Jgui3-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4): data_wrangling\n",
        "\n",
        "### Subtask:\n",
        "Convert qualitative variables into numerical representations using appropriate encoding techniques\n",
        "\n"
      ],
      "metadata": {
        "id": "QZ_4c9y6FBvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('/content/Randomized_Dataset.csv')\n",
        "\n",
        "\n",
        "# Display data types and check for missing values.\n",
        "print(df.info())\n",
        "\n",
        "# Get summary statistics for numerical columns.\n",
        "print(df.describe())\n",
        "\n",
        "# Identify qualitative variables.\n",
        "qualitative_vars = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Qualitative variables: {qualitative_vars}\")\n",
        "\n",
        "# Explore target variables.\n",
        "target_vars = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "for target in target_vars:\n",
        "    print(f\"\\n--- {target} ---\")\n",
        "    print(df[target].value_counts())\n",
        "    # If numerical, plot a histogram.\n",
        "    if df[target].dtype != 'object':\n",
        "        df[target].hist()\n",
        "        plt.title(f\"Distribution of {target}\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "9idAY0t3E984"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fill in missing data and convert qualitative variables to numeric"
      ],
      "metadata": {
        "id": "8L4nc__op4zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "# Create indicator column\n",
        "df['missing_indicator'] = ''\n",
        "# df['any_missing'] = 0  # Initialize 'any_missing' column to 0\n",
        "# Handle missing values (if any) - for simplicity, we'll fill with the mean for numerical and mode for categorical.\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    else:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "        df.loc[df.index, 'missing_indicator'] += col + ', '  # Use .loc to access the column correctly  # Concatenate column name\n",
        "#        df.loc[df.index, 'any_missing'] = 1  # Set 'any_missing' to 1 for rows with missing values\n",
        "\n",
        "# Convert qualitative variables to numerical using Label Encoding.\n",
        "qualitative_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "for col in qualitative_vars:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Ensure 'missing_indicator' column is of string type before applying str methods\n",
        "df['missing_indicator'] = df['missing_indicator'].astype(str)  # Convert to string type\n",
        "# Remove trailing delimiter if present\n",
        "df['missing_indicator'] = df['missing_indicator'].str.rstrip(', ')"
      ],
      "metadata": {
        "id": "5OYi6_HEK4dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[55:58:, 100:121]"
      ],
      "metadata": {
        "id": "Ntwgf17sBxjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the data in the four DataFrames (`df_major_cash`, `df_major_pledge`, `df_commit_major`, `df_inflation_major_commit`) to prepare them for model training. This involves dropping irrelevant columns and converting qualitative variables into numerical representations. Subsetting the data to preserve main dataset\n"
      ],
      "metadata": {
        "id": "GjAULoarqLS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_major_cash = df.copy()  # Creating a copy to avoid modifying the original DataFrame\n",
        "df_major_pledge = df.copy()\n",
        "df_commit_major = df.copy()\n",
        "df_inflation_major_commit = df.copy()\n",
        "\n",
        "# # Drop irrelevant columns\n",
        "df_major_cash = df_major_cash.drop(columns=['MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_major_pledge = df_major_pledge.drop(columns=['MAJOR_CASH', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_commit_major = df_commit_major.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_inflation_major_commit = df_inflation_major_commit.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR'])\n",
        "#\n",
        "# Convert qualitative variables to numerical representations using one-hot encoding\n",
        "def convert_qualitative_to_numerical(df):\n",
        "    qualitative_vars = df.select_dtypes(include=['object']).columns\n",
        "    df = pd.get_dummies(df, columns=qualitative_vars)\n",
        "    return df\n",
        "\n",
        "df_major_cash = convert_qualitative_to_numerical(df_major_cash)\n",
        "df_major_pledge = convert_qualitative_to_numerical(df_major_pledge)\n",
        "df_commit_major = convert_qualitative_to_numerical(df_commit_major)\n",
        "df_inflation_major_commit = convert_qualitative_to_numerical(df_inflation_major_commit)"
      ],
      "metadata": {
        "id": "rOqS8E6RFk8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MEAT AND POTATOES"
      ],
      "metadata": {
        "id": "HoKL_FYQEUWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loop through models\n",
        "1) RANDOM FOREST\n",
        "\n",
        "2) LOGISTIC REGRESSION\n",
        "\n",
        "3) SVC\n",
        "\n",
        "4) DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "yHaIqnqvDE37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " programmatically test a variety of models, explore, and compare the results in your Google Colab environment:"
      ],
      "metadata": {
        "id": "6LZuA92J4E0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's add the twist of sampling 50 variables 3 times for each model and compare the results. include a listing of the 50 variables used for each model and target variable, along with a comparison of the results.Here's the updated code:"
      ],
      "metadata": {
        "id": "wM5qtpo14Az3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of Exploring Results:"
      ],
      "metadata": {
        "id": "qeV-4hgy5cAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate average accuracy for RandomForest on 'MAJOR_CASH'\n",
        "rf_major_cash_accuracy = [result['accuracy'] for result in results['MAJOR_CASH']['RandomForestClassifier']]\n",
        "avg_accuracy = np.mean(rf_major_cash_accuracy)\n",
        "print(f\"Average accuracy for RandomForest on MAJOR_CASH: {avg_accuracy}\")"
      ],
      "metadata": {
        "id": "eV6GeqB45Eva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SvLG2hV25YbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the whole enchilada 🌮\n"
      ],
      "metadata": {
        "id": "3X_LZSeb7F2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "identify the best variables (features) for your models. We'll use feature importance scores provided by the models themselves (like RandomForest) or techniques like Recursive Feature Elimination (RFE) for models that don't directly provide feature importance."
      ],
      "metadata": {
        "id": "1Sn55PvS8olw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlltogetherNOW"
      ],
      "metadata": {
        "id": "fYNrh3UOKw56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**: Measures the overall correctness of predictions.Calculated as (number of correct predictions) / (total number of predictions).\n",
        "\n",
        "**Precision**: Measures the proportion of true positive predictions among all positive predictions. Useful when the cost of false positives is high.\n",
        "\n",
        "**Recall (Sensitivity)**: Measures the proportion of true positive predictions among all actual positives.\n",
        "Useful when the cost of false negatives is high.\n",
        "\n",
        "**F1-Score**: The harmonic mean of precision and recall.Provides a balance between the two metrics."
      ],
      "metadata": {
        "id": "B78XGQPOVl4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**\n",
        "\n",
        "A table that visualizes the performance of a classification model.\n",
        "Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "   import seaborn as sns\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   cm = confusion_matrix(y_true, y_pred)  \n",
        "   sns.heatmap(cm, annot=True, fmt='d')  \n",
        "   plt.xlabel('Predicted')\n",
        "   plt.ylabel('Actual')\n",
        "   plt.title('Confusion Matrix')\n",
        "   plt.show()\n",
        "\n",
        "**Classification Report**\n",
        "\n",
        "Provides a summary of key classification metrics (precision, recall, F1-score, support) for each class.\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "   report = classification_report(y_true, y_pred)\n",
        "   print(report)\n",
        "Use code with caution\n",
        "Incorporating into Your Code\n",
        "\n",
        "After Model Training: Calculate and print these metrics after training each of your models.\n",
        "Using y_true and y_pred: Ensure you have the actual target values (y_true) and the model's predictions (y_pred) to use in the metric calculations.\n",
        "Comparison: Compare the metrics across different models to understand their relative strengths and weaknesses.\n",
        "By systematically monitoring these performance metrics, you can gain valuable insights into your models' behavior and make informed decisions about model selection and improvement. Let me know if you'd like me to show you how to integrate these into your existing code!"
      ],
      "metadata": {
        "id": "EwBTR24SW8B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More all to gether"
      ],
      "metadata": {
        "id": "YyGMiAkndfFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier # Import MLP Classifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix, classification_report\n",
        "# from sklearn.feature_selection import RFE  # For Recursive Feature Elimination\n",
        "\n",
        "# Define a list of models to test\n",
        "models = [\n",
        "#    LogisticRegression(random_state=42),\n",
        "    LogisticRegression(random_state=42, solver='saga', max_iter=1000),\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    SVC(random_state=42),\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    MLPClassifier(random_state=42, hidden_layer_sizes=(100,), max_iter=500)  # Add MLP\n",
        "]\n",
        "\n",
        "# Define a dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Create a list to store all results for DataFrame\n",
        "all_results = []\n",
        "\n",
        "# Main loop through target variables, models, and sampling iterations\n",
        "for target_variable in ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']:\n",
        "    results[target_variable] = {}\n",
        "    for model in models:\n",
        "        model_name = type(model).__name__\n",
        "        results[target_variable][model_name] = []\n",
        "\n",
        "        for iteration in range(3):\n",
        "            # Get all features except target variable\n",
        "            all_features = [col for col in df.columns if col != target_variable]\n",
        "\n",
        "            # Randomly sample 50 features\n",
        "            sampled_features = np.random.choice(all_features, size=50, replace=False)\n",
        "\n",
        "            # Get the data for the current target variable and sampled features\n",
        "            X = df[sampled_features]\n",
        "            y = df[target_variable]\n",
        "\n",
        "            # Split the data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate evaluation metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "            recall = recall_score(y_test, y_pred, zero_division=1)\n",
        "            f1 = f1_score(y_test, y_pred, zero_division=1)\n",
        "\t\t\t# Calculate metrics\n",
        "            cm = confusion_matrix(y_test, y_pred)  # Get confusion matrix\n",
        "            report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)  # Get classification report as dict\n",
        "            perm_importance = permutation_importance(model, X_test, y_test, scoring='accuracy', random_state=42)\n",
        "\n",
        "\n",
        "            # Store results in the dictionary and list for DataFrame\n",
        "            results[target_variable][model_name].append({\n",
        "                'iteration': iteration + 1,\n",
        "                'features': sampled_features.tolist(),\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'perm_importance_mean': perm_importance.importances_mean.tolist(),  # Add importance\n",
        "        \t\t\t\t'cm':cm,\n",
        "\t\t\t\t        'report':report\n",
        "            })\n",
        "            all_results.append({\n",
        "                'target_variable': target_variable,\n",
        "                'model': model_name,\n",
        "                'iteration': iteration + 1,\n",
        "                'features': sampled_features.tolist(),\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "\t\t\t\t        'perm_importance_mean': perm_importance.importances_mean.tolist(),  # Store importance\n",
        "                'cm':cm,\n",
        "\t\t\t\t'report':report\n",
        "            })\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(results_df)\n",
        "\n",
        "        # Feature importance for models with built-in importance scores\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "            model.fit(X, y)  # Fit on all data for feature importance\n",
        "            importances = model.feature_importances_\n",
        "            indices = np.argsort(importances)[::-1]  # Sort feature indices by importance\n",
        "\n",
        "            print(f\"\\n-- {model_name} --\")\n",
        "            for f in range(X.shape[1]):\n",
        "                if importances[indices[f]] > 0:  # Print only features with importance > 0\n",
        "                   print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
        "\n",
        "# RFE for models without built-in importance scores\n",
        "else:\n",
        "    #print(f\"\\n-- {model_name} (using RFE) --\")\n",
        "    print(f\"\\n-- {model_name} (using Permutation Importance) --\")\n",
        "\n",
        "    # Explicitly set 'linear' kernel for SVC to enable coef_ attribute\n",
        "    if model_name == 'SVC':\n",
        "        estimator = SVC(kernel='linear', random_state=42)\n",
        "    else:\n",
        "        estimator = model  # Use the original model if not SVC\n",
        "\n",
        "  #  rfe = RFE(estimator=estimator, n_features_to_select=10)  # Select top 10 features\n",
        "  #  rfe.fit(X, y)\n",
        "\n",
        "      # Fit the model on all data for permutation importance\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Calculate permutation importance\n",
        "    perm_importance = permutation_importance(model, X, y, scoring='accuracy', random_state=42)\n",
        "\n",
        "    # Sort feature indices by importance\n",
        "    indices = np.argsort(perm_importance.importances_mean)[::-1]\n",
        "\n",
        "\n",
        "    # Print selected features\n",
        "    for f in range(X.shape[1]): # changed i to f\n",
        "      #  if rfe.support_[i]:\n",
        "       #     print(f\"{i + 1}. feature {X.columns[i]}\")\n",
        "         if perm_importance.importances_mean[indices[f]] > 0:  # Print only features with importance > 0\n",
        "            print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], perm_importance.importances_mean[indices[f]]))\n"
      ],
      "metadata": {
        "id": "eHlW78cvdRxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible Solutions\n",
        "\n",
        "Here are a few options to address this warning:\n",
        "\n",
        "Increase max_iter:\n",
        "The most straightforward approach is to increase the max_iter parameter in your LogisticRegression model. This allows the solver more iterations to try and converge.\n",
        "Try increasing it to a larger value, such as 10000 or even higher, and see if the warning disappears.\n",
        "Add the following code inside the loop after models are defined:\n",
        "\n",
        "if model_name == 'LogisticRegression':\n",
        "       model.set_params(max_iter=10000)  # Increase max_iter for Logistic Regression\n",
        "Use code with caution\n",
        "Adjust tol (tolerance):\n",
        "You can try decreasing the tol parameter, which controls the tolerance for stopping criteria. This might allow the solver to converge faster, but be careful as it could potentially lead to premature convergence.\n",
        "Experiment with smaller values, such as tol=1e-5 or tol=1e-6, and observe the effect on the warning and performance.\n",
        "Add the following code inside the loop after models are defined:\n",
        "\n",
        "if model_name == 'LogisticRegression':\n",
        "       model.set_params(tol=1e-5)  # Adjust tolerance\n",
        "Use code with caution\n",
        "Use a different solver:\n",
        "While 'saga' is generally a good choice for large datasets, you could try using other solvers like 'lbfgs' or 'liblinear' to see if they converge more easily for your specific data. Change the code to:\n",
        "\n",
        "models = [\n",
        "       LogisticRegression(random_state=42, solver='lbfgs', max_iter=1000), # changing solver from 'saga' to 'lbfgs'\n",
        "   #...(rest of your models)\n",
        "   ]\n",
        "Use code with caution\n",
        "Should you skip the model?\n",
        "\n",
        "If the warning persists even after trying the above solutions and your accuracy is poor, it might be worth considering skipping the LogisticRegression model with the 'saga' solver, especially if other models perform significantly better.\n",
        "However, if the accuracy is acceptable or comparable to other models despite the warning, it might be okay to keep it in the analysis. You can monitor the warning and make a judgment based on overall model performance.\n",
        "Incorporating the changes\n",
        "\n",
        "Apply the chosen solution(s) to the code snippet where you define the LogisticRegression model within your loop. Remember to track the warning messages and performance metrics to assess the effectiveness of your modifications.\n",
        "\n",
        "I recommend starting with increasing max_iter and then experimenting with adjusting tol if needed. If those don't resolve the issue, consider trying a different solver or potentially skipping the model depending on your accuracy goals.\n",
        "\n",
        "github.com/scikit-learn/scikit-learn/discussions/27062\n",
        "github.com/scikit-learn/scikit-learn/issues/27561\n",
        "stackoverflow.com/questions/46028914/multilayer-perceptron-convergencewarning-stochastic-optimizer-maximum-iterat\n",
        "stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
        "datascience.stackexchange.com/questions/38143/multilayer-perceptron-does-not-converge\n"
      ],
      "metadata": {
        "id": "R2R-6qC208TM"
      }
    },
    {
      "source": [
        "results_df.to_csv('results.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NSqNMskgPOVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tHE6-n8o1E4e"
      }
    }
  ]
}