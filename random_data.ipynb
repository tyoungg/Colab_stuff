{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyoungg/Colab_stuff/blob/main/random_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Predict each of these variables independently MAJOR_CASH, MAJOR_PLEDGE, COMMIT_MAJOR, INFLATION_MAJOR_COMMIT, some of the data may also require conversion from qualitative variables. By independent I mean I would like to not consider MAJOR_CASH, MAJOR_PLEDGE, COMMIT_MAJOR, INFLATION_MAJOR_COMMIT as part of the entire dataset\n",
        "\n",
        "Here is all the data you need:\n",
        "/tmp/Complete_Randomized_Dataset.csv"
      ],
      "metadata": {
        "id": "1WZRP7JL7_Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) data_loading\n",
        "\n",
        "### Subtask:\n",
        "Load the data from the provided CSV file into a pandas DataFrame."
      ],
      "metadata": {
        "id": "qjKFexPT8WKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan\n",
        "\n",
        "1. **data_loading**: Load the data SQL into a pandas DataFrame using `pd.read_csv()`.\n",
        "2. **data_exploration**:\n",
        "    - Use `df.info()` and `df.describe()` to understand the data types and distributions of each variable, including `MAJOR_CASH`, `MAJOR_PLEDGE`, `COMMIT_MAJOR`, and `INFLATION_MAJOR_COMMIT`.\n",
        "    - Identify qualitative variables that need conversion by checking their data types.\n",
        "3. **data_preparation**: Create four separate copies of the DataFrame using `df.copy()` for predicting each target variable (`MAJOR_CASH`, `MAJOR_PLEDGE`, `COMMIT_MAJOR`, `INFLATION_MAJOR_COMMIT`).\n",
        "4. **data_wrangling**:\n",
        "    - In each DataFrame copy, drop the other three target variables using `df.drop(columns=['column_name'])`.\n",
        "    - Convert qualitative variables into numerical representations using appropriate encoding techniques (e.g., `pd.get_dummies()` for one-hot encoding) within each DataFrame copy.\n",
        "5. **feature_engineering**:\n",
        "    - For each DataFrame copy, select relevant features for prediction. This may involve dropping irrelevant columns using `df.drop(columns=['column_name'])` or creating new features based on existing ones using Python 3 syntax.\n",
        "    - Handle missing values in the features using appropriate techniques (e.g., `df.fillna()` for imputation) within each DataFrame copy.\n",
        "6. **data_splitting**: For each DataFrame copy, split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`.\n",
        "7. **model_training**: Train a separate machine learning model (e.g., regression, classification) for each target variable using its corresponding DataFrame copy. Choose the model based on the nature of the target variable (continuous or categorical). Import necessary models from `sklearn` using Python 3 syntax.\n",
        "    - **model_training**: Train a model to predict `MAJOR_CASH` using the first DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `MAJOR_PLEDGE` using the second DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `COMMIT_MAJOR` using the third DataFrame copy.\n",
        "    - **model_training**: Train a model to predict `INFLATION_MAJOR_COMMIT` using the fourth DataFrame copy.\n",
        "8. **model_evaluation**: Evaluate the performance of each model on its corresponding testing set using appropriate metrics (e.g., R-squared, accuracy, precision, recall). Import necessary metrics from `sklearn.metrics` using Python 3 syntax.\n",
        "    - **model_evaluation**: Evaluate the `MAJOR_CASH` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `MAJOR_PLEDGE` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `COMMIT_MAJOR` prediction model.\n",
        "    - **model_evaluation**: Evaluate the `INFLATION_MAJOR_COMMIT` prediction model.\n",
        "9. **finish_task**: Write a summary report describing the process, the models trained for each target variable, their performance, and any insights gained from the analysis. Include recommendations for future work."
      ],
      "metadata": {
        "id": "_fuZlgQw8NOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install necessary packages"
      ],
      "metadata": {
        "id": "x2OOy6mM8t00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker\n",
        "!pip install scipy\n",
        "!pip install scikit-learn #Install scikit-learn if not already installed"
      ],
      "metadata": {
        "id": "Wq6x0jf8rfnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Random test data"
      ],
      "metadata": {
        "id": "CG_QFfYI9Fft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addtion creating correlations of:\n",
        "FRIEND being 65 percent correlated to major gift = yes\n",
        "ATTENDED_EVENT being 25 perscent negatively correlated with major gift = yes\n",
        "Distance being 75 percent positively correlated with major gift = yes"
      ],
      "metadata": {
        "id": "edr4c6dX0ZRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Initialize Faker and constants\n",
        "fake = Faker()\n",
        "num_rows = 10000\n",
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\",\n",
        "    \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\",\n",
        "    \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\",\n",
        "    \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n",
        "    \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "# Define base probabilities and relationships\n",
        "major_cash_probs = norm.cdf(np.random.randn(num_rows))  # N(0, 1), transformed to probabilities\n",
        "major_cash = np.where(major_cash_probs > 0.5, \"yes\", \"no\")\n",
        "\n",
        "# FRIEND correlated with MAJOR_CASH (65%)\n",
        "friend_probs = norm.cdf(np.random.randn(num_rows) + 0.65 * (major_cash == \"yes\"))\n",
        "friend = np.where(friend_probs > 0.5, \"yes\", \"no\")\n",
        "\n",
        "# ATTENDED_EVENT negatively correlated with MAJOR_CASH (25%)\n",
        "attended_event_probs = norm.cdf(np.random.randn(num_rows) - 0.25 * (major_cash == \"yes\"))\n",
        "attended_event = np.where(attended_event_probs > 0.5, \"yes\", \"no\")\n",
        "\n",
        "# DISTANCE positively correlated with MAJOR_CASH (75%)\n",
        "distance_mean = np.where(major_cash == \"yes\", 3000, 1000)  # Higher mean when MAJOR_CASH is \"yes\"\n",
        "distance_std = 1000  # Standard deviation\n",
        "distance = np.clip(norm.rvs(loc=distance_mean, scale=distance_std), 10, 4500)  # Clip range\n",
        "\n",
        "\n",
        "\n",
        "# Generate random data for all fields\n",
        "data = {\n",
        "    \"MAJOR_CASH\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MAJOR_PLEDGE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COMMIT_MAJOR\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"INFLATION_MAJOR_COMMIT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_COMMIT_VALUE\": [random.uniform(1000, 100000) for _ in range(num_rows)],\n",
        "    \"BASELINE\": [random.uniform(500, 50000) for _ in range(num_rows)],\n",
        "    \"PRINCIPAL_GIFT\": [random.uniform(100, 10000) for _ in range(num_rows)],\n",
        "    \"BELOW_BASELINE\": [random.uniform(0, 5000) for _ in range(num_rows)],\n",
        "    \"LEADERSHIP_GIFT\": [random.uniform(5000, 50000) for _ in range(num_rows)],\n",
        "    \"BASELINE_NO_LEADERSHIP\": [random.uniform(500, 5000) for _ in range(num_rows)],\n",
        "    \"LEGAL_CREDIT\": [random.uniform(100, 100000) for _ in range(num_rows)],\n",
        "    \"CASH\": [random.uniform(0, 50000) for _ in range(num_rows)],\n",
        "    \"PLEDGE\": [random.uniform(0, 30000) for _ in range(num_rows)],\n",
        "    \"DEFERRED\": [random.uniform(0, 10000) for _ in range(num_rows)],\n",
        "    \"CASH_RECEIVED\": [random.uniform(0, 50000) for _ in range(num_rows)],\n",
        "    \"OUTSTANDING_BALANCE\": [random.uniform(0, 20000) for _ in range(num_rows)],\n",
        "    \"NON_GIFT\": [random.uniform(0, 10000) for _ in range(num_rows)],\n",
        "    \"DATE_FIRST_GIFT\": [fake.date_between(start_date=\"-30y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"FIRST_GIFT_AMOUNT\": [random.uniform(50, 10000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJUSTED_FIRST_AMOUNT\": [random.uniform(50, 15000) for _ in range(num_rows)],\n",
        "    \"DATE_LAST_GIFT\": [fake.date_between(start_date=\"-5y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_AMOUNT\": [random.uniform(50, 10000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJ_LAST_AMOUNT\": [random.uniform(50, 15000) for _ in range(num_rows)],\n",
        "    \"LARGEST_DATE\": [fake.date_between(start_date=\"-10y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LARGEST_AMOUNT\": [random.uniform(100, 20000) for _ in range(num_rows)],\n",
        "    \"INFLATION_ADJ_LARGEST_AMOUNT\": [random.uniform(100, 25000) for _ in range(num_rows)],\n",
        "    \"PRIMARY_UNIT\": [f\"Unit_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"PRIMARY_UNIT_LIFETIME_FUNDRAISING\": [random.uniform(10000, 500000) for _ in range(num_rows)],\n",
        "    \"TOTAL_YEARS_GIVING\": [random.randint(1, 40) for _ in range(num_rows)],\n",
        "    \"RECENT_CONSECUTIVE_STREAK_GIVING\": [random.randint(1, 10) for _ in range(num_rows)],\n",
        "    \"SEGMENT\": [f\"Segment_{random.randint(1, 10)}\" for _ in range(num_rows)],\n",
        "    \"LONGEST_CONSECUTIVE_STREAK\": [random.randint(1, 20) for _ in range(num_rows)],\n",
        "    \"TERMS_ATTENDED\": [random.randint(3, 18) for _ in range(num_rows)],\n",
        "    \"HOURS_ATTENDED\": [random.randint(0, 8000) for _ in range(num_rows)],\n",
        "    \"AGE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "   # \"AGE_CAT_BY_10\": [f\"{age // 10 * 10}s\" for age in data[\"AGE\"]],\n",
        "    \"ANONYMOUS_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"RECORD_STATUS\": [fake.word() for _ in range(num_rows)],\n",
        "    \"ENROLLED_YEAR\": [random.randint(1945, 2020) for _ in range(num_rows)],\n",
        "    \"ENROLLED_SCHOOL\": [f\"School_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"IS_FIRST_GEN_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FIRST_DEGREE_YEAR\": [random.randint(1950, 2020) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_LAST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_FIRST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"FIRST_LAST_DEGREE_DIFF\": [random.uniform(0, 10) for _ in range(num_rows)],\n",
        "    \"Age_at_FIRST_DEGREE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "    \"P_COUNTRY\": [random.choice([\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"Japan\", \"India\", \"Australia\"]) for _ in range(num_rows)],\n",
        "#     \"P_STATE\": [random.choice(us_states) if country == \"USA\" else None for country in data[\"P_COUNTRY\"]],\n",
        "     \"ALUMNUS_DEGREED\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ALUMNUS_NONDEGREED\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXTERNAL_CONTACT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FORMER_EMPLOYEE_ALL\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FORMER_SPECIFIC_EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"FRIEND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"HOUSESTAFF\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"SPECIFC_EMPLOYEE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"SPECIFIC_FRIEND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ANONYMOUS_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"RECORD_STATUS\": [fake.word() for _ in range(num_rows)],\n",
        "    \"ENROLLED_YEAR\": [random.randint(1945, 2020) for _ in range(num_rows)],\n",
        "    \"ENROLLED_SCHOOL\": [f\"School_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"IS_FIRST_GEN_YN\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"FIRST_DEGREE_YEAR\": [random.randint(1950, 2020) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_LAST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"YEARS_SINCE_FIRST_DEGREE\": [random.uniform(0, 70) for _ in range(num_rows)],\n",
        "    \"FIRST_LAST_DEGREE_DIFF\": [random.uniform(0, 10) for _ in range(num_rows)],\n",
        "    \"Age_at_FIRST_DEGREE\": [random.randint(20, 70) for _ in range(num_rows)],\n",
        "    \"SPOUSE_IS_DECEASED_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MARITAL_STATUS\": [random.choice([\"Single\", \"Married\", \"Divorced\"]) for _ in range(num_rows)],\n",
        "    \"SOLICITABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PHONABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"MAILABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EMAILABLE_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"GDPR_HOLD_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOP_MANAGER_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"LAST_SUBST_CONTACT_DATE\": [fake.date_between(start_date=\"-5y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_SUBST_CONTACT_UNIT\": [f\"Unit_{random.randint(1, 15)}\" for _ in range(num_rows)],\n",
        "    \"STAGE_OF_READINESS\": [f\"Stage_{random.randint(1, 9)}\" for _ in range(num_rows)],\n",
        "    \"PROPOSAL_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"WEALTH_RATING\": [random.randint(0, 20) for _ in range(num_rows)],\n",
        "    \"PARENT_OF_CURRENT_STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT_FIRST_TIME_IN_COLLEGE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PARENT_CURRENT_UNDERGRAD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ENROLLED_CHILDREN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COUNT_ENROLLED_CHILDREN\": [random.randint(0, 4) for _ in range(num_rows)],\n",
        "    \"PARENT_HONORS\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"DEGREE_COUNT\": [random.randint(0, 5) for _ in range(num_rows)],\n",
        "    \"P_STATE\": [fake.state_abbr() for _ in range(num_rows)],\n",
        "    \"P_COUNTRY\": [random.choice([\"USA\", \"Canada\", \"UK\", \"Germany\", \"France\", \"Japan\", \"India\", \"Australia\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_COMMITS\": [random.randint(0, 100) for _ in range(num_rows)],\n",
        "    \"LEGACY_SOCIETY\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRESIDENTS_COUNCIL\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRESIDENTIAL_PROSPECTS\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"PRINCIPAL_GIFTS\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"CELEBRITY\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXEC_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NATIONAL_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"LIFE_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"A_BOARD\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"PHILANTHROPIC_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ENGAGEMENT_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"EXPERIENTIAL_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"COMMUNICATION_FLAG\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_CREDENTIALS\": [random.randint(0, 7) for _ in range(num_rows)],\n",
        "    \"LAST_ACTIVITY_DATE\": [fake.date_between(start_date=\"-3y\", end_date=\"today\") for _ in range(num_rows)],\n",
        "    \"LAST_ACTIVITY_TYPE\": [fake.word() for _ in range(num_rows)],\n",
        "    \"IS_LEGACY_STUDENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"IS_ADV_BOARD_MEMBER\": [random.choice([True, False]) for _ in range(num_rows)],\n",
        "    \"IS_ALUM_BOARD_MEMBER\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"ATTENDED_EVENT\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"TOTAL_EVENTS_ATTENDED\": [random.randint(0, 23) for _ in range(num_rows)],\n",
        "    \"FRAT_OR_SOROR\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"HONOR_SOCIETY_IND\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NUM_AWARDS_RCVD\": [random.randint(0, 6) for _ in range(num_rows)],\n",
        "    \"NUM_ACTIVE_PLEDGES\": [random.randint(0, 3) for _ in range(num_rows)],\n",
        "    \"GIVING_SOCIETY\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        " #   \"DISTANCE\": [random.uniform(10, 4500) if country == \"USA\" else None for country in data[\"P_COUNTRY\"]],\n",
        "   \"A_MEMB_YN\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"A_MEMB_TYPE\": [random.choice([\"yes\", \"no\"]) for _ in range(num_rows)],\n",
        "    \"NUM_OPEN_PROPOSALS\": [random.randint(0, 3) for _ in range(num_rows)],\n",
        "    \"MG_MODEL_SCORE\": [random.uniform(0, 5) for _ in range(num_rows)]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "import random\n",
        "df['DISTANCE'] = [random.uniform(10, 4500) if c == \"USA\" else None for c in df['P_COUNTRY']]\n",
        "df['P_STATE'] = [random.choice(us_states) if country == \"USA\" else None for country in df['P_COUNTRY']]\n",
        "df['AGE_CAT_BY_10'] = [f\"{age // 10 * 10}s\" for age in df['AGE']]\n",
        "# Set the index to include new variables\n",
        "# df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "df = df.reset_index(drop=True)\n",
        "df =  pd.concat([df, pd.DataFrame({\n",
        "     \"MAJOR_CASH_C\": major_cash,\n",
        "    \"FRIEND_C\": friend,\n",
        "    \"ATTENDED_EVENT_C\": attended_event,\n",
        "    \"DISTANCE_C\": distance\n",
        "})], axis=1)\n",
        "\n",
        "\n",
        "# Reset the index and update columns\n",
        "df = df.reset_index()\n",
        "df.columns  # Force recalculation of columns attribute\n",
        "\n",
        "# df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "# Save to CSV\n",
        "# output_path = \"/content/Randomized_Dataset.csv\"\n",
        "# df.to_csv(output_path, index=True)\n",
        "\n",
        "# print(f\"Dataset with {len(data)} fields saved to {output_path}\")\n",
        "# df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "W3hnb15-rG-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for all columns\n",
        "df.shape[1]\n"
      ],
      "metadata": {
        "id": "VVrHvjNDA-ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#create list of columns names\n",
        "for column manipulation\n"
      ],
      "metadata": {
        "id": "NpCiYjrT9Wzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = df.columns\n",
        "\n",
        "# Convert the column names to a list\n",
        "column_list = df.columns.tolist()\n",
        "\n",
        "print(column_names)\n",
        "print(column_list)"
      ],
      "metadata": {
        "id": "JbXntlNz-O_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General review of data\n"
      ],
      "metadata": {
        "id": "2RApdVkXIOtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 1. Data Shape\n",
        "print(f\"Data Shape: {df.shape}\")\n",
        "\n",
        "# 2. Data Types\n",
        "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "\n",
        "# 3. Descriptive Statistics\n",
        "print(f\"\\nDescriptive Statistics:\\n{df.describe()}\")\n",
        "\n",
        "# 4. Unique Values\n",
        "# for col in ['bird_name', 'device_info_serial']:\n",
        "#    print(f\"\\nUnique Values for {col}:\\n{df[col].value_counts()}\")\n",
        "\n",
        "# 5. Missing Values\n",
        "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# 6. Correlations\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate correlations on the numeric DataFrame\n",
        "print(f\"\\nCorrelations:\\n{numeric_df.corr()}\")\n"
      ],
      "metadata": {
        "id": "6D-FE7Jgui3-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4): data_wrangling\n",
        "\n",
        "### Subtask:\n",
        "Convert qualitative variables into numerical representations using appropriate encoding techniques\n",
        "\n"
      ],
      "metadata": {
        "id": "QZ_4c9y6FBvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('/content/Randomized_Dataset.csv')\n",
        "\n",
        "\n",
        "# Display data types and check for missing values.\n",
        "print(df.info())\n",
        "\n",
        "# Get summary statistics for numerical columns.\n",
        "print(df.describe())\n",
        "\n",
        "# Identify qualitative variables.\n",
        "qualitative_vars = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Qualitative variables: {qualitative_vars}\")\n",
        "\n",
        "# Explore target variables.\n",
        "target_vars = ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n",
        "for target in target_vars:\n",
        "    print(f\"\\n--- {target} ---\")\n",
        "    print(df[target].value_counts())\n",
        "    # If numerical, plot a histogram.\n",
        "    if df[target].dtype != 'object':\n",
        "        df[target].hist()\n",
        "        plt.title(f\"Distribution of {target}\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "9idAY0t3E984"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fill in missing data and convert qualitative variables to numeric"
      ],
      "metadata": {
        "id": "8L4nc__op4zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "# Create indicator column\n",
        "df['missing_indicator'] = ''\n",
        "# df['any_missing'] = 0  # Initialize 'any_missing' column to 0\n",
        "# Handle missing values (if any) - for simplicity, we'll fill with the mean for numerical and mode for categorical.\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    else:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "        df.loc[df.index, 'missing_indicator'] += col + ', '  # Use .loc to access the column correctly  # Concatenate column name\n",
        "#        df.loc[df.index, 'any_missing'] = 1  # Set 'any_missing' to 1 for rows with missing values\n",
        "\n",
        "# Convert qualitative variables to numerical using Label Encoding.\n",
        "qualitative_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "for col in qualitative_vars:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Ensure 'missing_indicator' column is of string type before applying str methods\n",
        "df['missing_indicator'] = df['missing_indicator'].astype(str)  # Convert to string type\n",
        "# Remove trailing delimiter if present\n",
        "df['missing_indicator'] = df['missing_indicator'].str.rstrip(', ')"
      ],
      "metadata": {
        "id": "5OYi6_HEK4dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[55:58:, 100:121]"
      ],
      "metadata": {
        "id": "Ntwgf17sBxjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the data in the four DataFrames (`df_major_cash`, `df_major_pledge`, `df_commit_major`, `df_inflation_major_commit`) to prepare them for model training. This involves dropping irrelevant columns and converting qualitative variables into numerical representations. Subsetting the data to preserve main dataset\n"
      ],
      "metadata": {
        "id": "GjAULoarqLS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_major_cash = df.copy()  # Creating a copy to avoid modifying the original DataFrame\n",
        "df_major_pledge = df.copy()\n",
        "df_commit_major = df.copy()\n",
        "df_inflation_major_commit = df.copy()\n",
        "\n",
        "# # Drop irrelevant columns\n",
        "df_major_cash = df_major_cash.drop(columns=['MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_major_pledge = df_major_pledge.drop(columns=['MAJOR_CASH', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_commit_major = df_commit_major.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'INFLATION_MAJOR_COMMIT'])\n",
        "df_inflation_major_commit = df_inflation_major_commit.drop(columns=['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR'])\n",
        "#\n",
        "# Convert qualitative variables to numerical representations using one-hot encoding\n",
        "def convert_qualitative_to_numerical(df):\n",
        "    qualitative_vars = df.select_dtypes(include=['object']).columns\n",
        "    df = pd.get_dummies(df, columns=qualitative_vars)\n",
        "    return df\n",
        "\n",
        "df_major_cash = convert_qualitative_to_numerical(df_major_cash)\n",
        "df_major_pledge = convert_qualitative_to_numerical(df_major_pledge)\n",
        "df_commit_major = convert_qualitative_to_numerical(df_commit_major)\n",
        "df_inflation_major_commit = convert_qualitative_to_numerical(df_inflation_major_commit)"
      ],
      "metadata": {
        "id": "rOqS8E6RFk8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MEAT AND POTATOES"
      ],
      "metadata": {
        "id": "HoKL_FYQEUWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loop through models\n",
        "1) RANDOM FOREST\n",
        "\n",
        "2) LOGISTIC REGRESSION\n",
        "\n",
        "3) SVC\n",
        "\n",
        "4) DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "yHaIqnqvDE37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " programmatically test a variety of models, explore, and compare the results in your Google Colab environment:"
      ],
      "metadata": {
        "id": "6LZuA92J4E0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's add the twist of sampling 50 variables 3 times for each model and compare the results. include a listing of the 50 variables used for each model and target variable, along with a comparison of the results.Here's the updated code:"
      ],
      "metadata": {
        "id": "wM5qtpo14Az3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of Exploring Results:"
      ],
      "metadata": {
        "id": "qeV-4hgy5cAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate average accuracy for RandomForest on 'MAJOR_CASH'\n",
        "rf_major_cash_accuracy = [result['accuracy'] for result in results['MAJOR_CASH']['RandomForestClassifier']]\n",
        "avg_accuracy = np.mean(rf_major_cash_accuracy)\n",
        "print(f\"Average accuracy for RandomForest on MAJOR_CASH: {avg_accuracy}\")"
      ],
      "metadata": {
        "id": "eV6GeqB45Eva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SvLG2hV25YbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the whole enchilada 🌮\n"
      ],
      "metadata": {
        "id": "3X_LZSeb7F2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "identify the best variables (features) for your models. We'll use feature importance scores provided by the models themselves (like RandomForest) or techniques like Recursive Feature Elimination (RFE) for models that don't directly provide feature importance."
      ],
      "metadata": {
        "id": "1Sn55PvS8olw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlltogetherNOW"
      ],
      "metadata": {
        "id": "fYNrh3UOKw56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**: Measures the overall correctness of predictions.Calculated as (number of correct predictions) / (total number of predictions).\n",
        "\n",
        "**Precision**: Measures the proportion of true positive predictions among all positive predictions. Useful when the cost of false positives is high.\n",
        "\n",
        "**Recall (Sensitivity)**: Measures the proportion of true positive predictions among all actual positives.\n",
        "Useful when the cost of false negatives is high.\n",
        "\n",
        "**F1-Score**: The harmonic mean of precision and recall.Provides a balance between the two metrics."
      ],
      "metadata": {
        "id": "B78XGQPOVl4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**\n",
        "\n",
        "A table that visualizes the performance of a classification model.\n",
        "Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "   import seaborn as sns\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   cm = confusion_matrix(y_true, y_pred)  \n",
        "   sns.heatmap(cm, annot=True, fmt='d')  \n",
        "   plt.xlabel('Predicted')\n",
        "   plt.ylabel('Actual')\n",
        "   plt.title('Confusion Matrix')\n",
        "   plt.show()\n",
        "\n",
        "**Classification Report**\n",
        "\n",
        "Provides a summary of key classification metrics (precision, recall, F1-score, support) for each class.\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "   report = classification_report(y_true, y_pred)\n",
        "   print(report)\n",
        "Use code with caution\n",
        "Incorporating into Your Code\n",
        "\n",
        "After Model Training: Calculate and print these metrics after training each of your models.\n",
        "Using y_true and y_pred: Ensure you have the actual target values (y_true) and the model's predictions (y_pred) to use in the metric calculations.\n",
        "Comparison: Compare the metrics across different models to understand their relative strengths and weaknesses.\n",
        "By systematically monitoring these performance metrics, you can gain valuable insights into your models' behavior and make informed decisions about model selection and improvement. Let me know if you'd like me to show you how to integrate these into your existing code!"
      ],
      "metadata": {
        "id": "EwBTR24SW8B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More all to gether"
      ],
      "metadata": {
        "id": "YyGMiAkndfFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier # Import MLP Classifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix, classification_report\n",
        "# from sklearn.feature_selection import RFE  # For Recursive Feature Elimination\n",
        "\n",
        "# Define a list of models to test\n",
        "models = [\n",
        "#    LogisticRegression(random_state=42),\n",
        "    LogisticRegression(random_state=42, solver='saga', max_iter=1000),\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    SVC(random_state=42),\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    MLPClassifier(random_state=42, hidden_layer_sizes=(100,), max_iter=500)  # Add MLP\n",
        "]\n",
        "\n",
        "# Define a dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Create a list to store all results for DataFrame\n",
        "all_results = []\n",
        "\n",
        "# Main loop through target variables, models, and sampling iterations\n",
        "for target_variable in ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']:\n",
        "    results[target_variable] = {}\n",
        "    for model in models:\n",
        "        model_name = type(model).__name__\n",
        "        results[target_variable][model_name] = []\n",
        "\n",
        "        for iteration in range(3):\n",
        "            # Get all features except target variable\n",
        "            all_features = [col for col in df.columns if col != target_variable]\n",
        "\n",
        "            # Randomly sample 50 features\n",
        "#            sampled_features = np.random.choice(all_features, size=50, replace=False)\n",
        "            sampled_features = np.random.choice(all_features, size=20, replace=False) #reduce size\n",
        "                   # Get the data for the current target variable and sampled features\n",
        "            X = df[sampled_features].copy()  # Create a copy to avoid modifying the original DataFrame\n",
        "\n",
        "        # Convert 'AGE_CAT_BY_10' to float if it's in sampled_features\n",
        "            if 'AGE_CAT_BY_10' in sampled_features:\n",
        "              X['AGE_CAT_BY_10'] = X['AGE_CAT_BY_10'].str.rstrip('s').astype(float)\n",
        "\n",
        "\n",
        "            # Get the data for the current target variable and sampled features\n",
        "     #       X = df[sampled_features]\n",
        "            y = df[target_variable]\n",
        "\n",
        "            # Split the data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate evaluation metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "            recall = recall_score(y_test, y_pred, zero_division=1)\n",
        "            f1 = f1_score(y_test, y_pred, zero_division=1)\n",
        "\t\t\t# Calculate metrics\n",
        "            cm = confusion_matrix(y_test, y_pred)  # Get confusion matrix\n",
        "            report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)  # Get classification report as dict\n",
        "            perm_importance = permutation_importance(model, X_test, y_test, scoring='accuracy', random_state=42)\n",
        "\n",
        "\n",
        "            # Store results in the dictionary and list for DataFrame\n",
        "            results[target_variable][model_name].append({\n",
        "                'iteration': iteration + 1,\n",
        "                'features': sampled_features.tolist(),\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'perm_importance_mean': perm_importance.importances_mean.tolist(),  # Add importance\n",
        "        \t\t\t\t'cm':cm,\n",
        "\t\t\t\t        'report':report\n",
        "            })\n",
        "            all_results.append({\n",
        "                'target_variable': target_variable,\n",
        "                'model': model_name,\n",
        "                'iteration': iteration + 1,\n",
        "                'features': sampled_features.tolist(),\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "\t\t\t\t        'perm_importance_mean': perm_importance.importances_mean.tolist(),  # Store importance\n",
        "                'cm':cm,\n",
        "\t\t\t\t'report':report\n",
        "            })\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "            results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Display the DataFrame\n",
        "            print(results_df)\n",
        "\n",
        "        # Feature importance for models with built-in importance scores\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "            model.fit(X, y)  # Fit on all data for feature importance\n",
        "            importances = model.feature_importances_\n",
        "            indices = np.argsort(importances)[::-1]  # Sort feature indices by importance\n",
        "\n",
        "            print(f\"\\n-- {model_name} --\")\n",
        "            for f in range(X.shape[1]):\n",
        "                if importances[indices[f]] > 0:  # Print only features with importance > 0\n",
        "                   print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
        "\n",
        "# RFE for models without built-in importance scores\n",
        "else:\n",
        "    #print(f\"\\n-- {model_name} (using RFE) --\")\n",
        "    print(f\"\\n-- {model_name} (using Permutation Importance) --\")\n",
        "\n",
        "    # Explicitly set 'linear' kernel for SVC to enable coef_ attribute\n",
        "    if model_name == 'SVC':\n",
        "        estimator = SVC(kernel='linear', random_state=42)\n",
        "    else:\n",
        "        estimator = model  # Use the original model if not SVC\n",
        "\n",
        "  #  rfe = RFE(estimator=estimator, n_features_to_select=10)  # Select top 10 features\n",
        "  #  rfe.fit(X, y)\n",
        "\n",
        "      # Fit the model on all data for permutation importance\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Calculate permutation importance\n",
        "    perm_importance = permutation_importance(model, X, y, scoring='accuracy', random_state=42)\n",
        "\n",
        "    # Sort feature indices by importance\n",
        "    indices = np.argsort(perm_importance.importances_mean)[::-1]\n",
        "\n",
        "\n",
        "    # Print selected features\n",
        "    for f in range(X.shape[1]): # changed i to f\n",
        "      #  if rfe.support_[i]:\n",
        "       #     print(f\"{i + 1}. feature {X.columns[i]}\")\n",
        "         if perm_importance.importances_mean[indices[f]] > 0:  # Print only features with importance > 0\n",
        "            print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], perm_importance.importances_mean[indices[f]]))\n"
      ],
      "metadata": {
        "id": "eHlW78cvdRxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a test dataset to test the model"
      ],
      "metadata": {
        "id": "PY_sk4SCkKlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame(data)\n",
        "import random\n",
        "df_test['DISTANCE'] = [random.uniform(10, 4500) if c == \"USA\" else None for c in df['P_COUNTRY']]\n",
        "df_test['P_STATE'] = [random.choice(us_states) if country == \"USA\" else None for country in df['P_COUNTRY']]\n",
        "df['AGE_CAT_BY_10'] = [f\"{age // 10 * 10}s\" for age in df_test['AGE']]\n",
        "# Set the index to include new variables\n",
        "# df = df.set_index(['DISTANCE', 'P_STATE', 'AGE_CAT_BY_10'])\n",
        "df_test = df.reset_index(drop=True)\n",
        "df_test =  pd.concat([df_test, pd.DataFrame({\n",
        "     \"MAJOR_CASH_C\": major_cash,\n",
        "    \"FRIEND_C\": friend,\n",
        "    \"ATTENDED_EVENT_C\": attended_event,\n",
        "    \"DISTANCE_C\": distance\n",
        "})], axis=1)\n",
        "\n",
        "\n",
        "# Reset the index and update columns\n",
        "df_test = df.reset_index()\n",
        "df_test.columns"
      ],
      "metadata": {
        "id": "89lPyLgwjxvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the new data\n",
        "new_data =df_test\n",
        "\n",
        "# Preprocess the new data\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "# Create indicator column\n",
        "new_data['missing_indicator'] = ''\n",
        "for col in new_data.columns:\n",
        "    if new_data[col].dtype == 'object':\n",
        "        new_data[col] = new_data[col].fillna(new_data[col].mode()[0])\n",
        "    else:\n",
        "        new_data[col] = new_data[col].fillna(new_data[col].mean())\n",
        "        new_data.loc[new_data.index, 'missing_indicator'] += col + ', '  # Use .loc to access the column correctly  # Concatenate column name\n",
        "\n",
        "\n",
        "# Convert qualitative variables to numerical using Label Encoding.\n",
        "qualitative_vars = new_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "for col in qualitative_vars:\n",
        "    le = LabelEncoder()\n",
        "    new_data[col] = le.fit_transform(new_data[col])\n",
        "\n",
        "# Ensure 'missing_indicator' column is of string type before applying str methods\n",
        "new_data['missing_indicator'] = new_data['missing_indicator'].astype(str)  # Convert to string type\n",
        "# Remove trailing delimiter if present\n",
        "new_data['missing_indicator'] = new_data['missing_indicator'].str.rstrip(', ')\n",
        "\n",
        "\n",
        "# Define the models (renamed to model_dict)\n",
        "model_dict = { # Renamed from 'models' to 'model_dict'\n",
        "    'LogisticRegression': LogisticRegression(random_state=42),\n",
        "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
        "    'SVC': SVC(random_state=42),\n",
        "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
        "    'MLPClassifier': MLPClassifier(random_state=42, hidden_layer_sizes=(100,), max_iter=500)\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Define a dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Create a list to store all results for DataFrame\n",
        "all_results = []\n",
        "\n",
        "# Create a dictionary to store the scores\n",
        "# Store scores\n",
        "scores = {}\n",
        "\n",
        "# Loop through targets and models\n",
        "# Save the trained models\n",
        "for target_variable in ['MAJOR_CASH', 'MAJOR_PLEDGE', 'COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']:\n",
        "    scores[target_variable] = {} # Initialize the inner dictionary for each target variable\n",
        "    for model_name, model in model_dict.items():  # Iterate through model_dict\n",
        "        for iteration in range(3):  # Loop for 3 iterations\n",
        "        # Get the features used for this model and target\n",
        "        # Filter results_df to find the features used for this target and model\n",
        "            filtered_df = results_df[(results_df['target_variable'] == target_variable) & (results_df['model'] == model_name)]\n",
        "\n",
        "        # Check if any rows match the filter\n",
        "            if not filtered_df.empty:\n",
        "                features = filtered_df['features'].values[0]  # Get features from the first matching row\n",
        "\n",
        "        # Exclude 'AGE_CAT_BY_10' from features if it's present\n",
        "                if 'AGE_CAT_BY_10' in features:\n",
        "                    features.remove('AGE_CAT_BY_10')\n",
        "            # Rest of your model training and evaluation code here...\n",
        "        # Train the model on the full dataset with the selected features\n",
        "            X = df[features]\n",
        "            y = df[target_variable]\n",
        "            model.fit(X, y)  # Fit the model\n",
        "\n",
        "        # Save the model\n",
        "            filename = f\"{target_variable}_{model_name.replace(' ', '_')}_{iteration + 1}.pkl\"  # Replace spaces in model name\n",
        "            with open(filename, 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "            print(f\"Model saved as {filename}\")\n",
        "\n",
        "            # Load the model # This line is added to load the model\n",
        "            with open(filename, 'rb') as f:\n",
        "                    loaded_model = pickle.load(f)\n",
        "\n",
        "        # Score the new data\n",
        "            new_predictions = loaded_model.predict(df_test[results_df.loc[(results_df['target_variable'] == target_variable) & (results_df['model'] == model_name), 'features'].iloc[0]])\n",
        "\n",
        "\n",
        "        # Calculate the evaluation metrics\n",
        "            accuracy = accuracy_score(new_data[target_variable], new_predictions)\n",
        "            precision = precision_score(new_data[target_variable], new_predictions)\n",
        "            recall = recall_score(new_data[target_variable], new_predictions)\n",
        "            f1 = f1_score(new_data[target_variable], new_predictions)\n",
        "\n",
        "        # Store the scores\n",
        "            scores[target_variable][f\"{model_name}_{iteration + 1}\"] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "        }\n",
        "\n",
        "# Print the scores\n",
        "for target_variable, model_scores in scores.items():\n",
        "       print(f\"Target variable: {target_variable}\")\n",
        "       for model_iteration_name, scores in model_scores.items():\n",
        "           model_name, iteration = model_iteration_name.rsplit('_', 1) # Extract model name and iteration\n",
        "           print(f\"{model_name} (Iteration {iteration}):\") # Include iteration in print\n",
        "           for metric, score in scores.items():\n",
        "               print(f\"  {metric}: {score:.2f}\")\n",
        "       print()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lU86Zs7b_y9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1d7Z9-F6S1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible Solutions\n",
        "\n",
        "Here are a few options to address this warning:\n",
        "\n",
        "Increase max_iter:\n",
        "The most straightforward approach is to increase the max_iter parameter in your LogisticRegression model. This allows the solver more iterations to try and converge.\n",
        "Try increasing it to a larger value, such as 10000 or even higher, and see if the warning disappears.\n",
        "Add the following code inside the loop after models are defined:\n",
        "\n",
        "if model_name == 'LogisticRegression':\n",
        "       model.set_params(max_iter=10000)  # Increase max_iter for Logistic Regression\n",
        "Use code with caution\n",
        "Adjust tol (tolerance):\n",
        "You can try decreasing the tol parameter, which controls the tolerance for stopping criteria. This might allow the solver to converge faster, but be careful as it could potentially lead to premature convergence.\n",
        "Experiment with smaller values, such as tol=1e-5 or tol=1e-6, and observe the effect on the warning and performance.\n",
        "Add the following code inside the loop after models are defined:\n",
        "\n",
        "if model_name == 'LogisticRegression':\n",
        "       model.set_params(tol=1e-5)  # Adjust tolerance\n",
        "Use code with caution\n",
        "Use a different solver:\n",
        "While 'saga' is generally a good choice for large datasets, you could try using other solvers like 'lbfgs' or 'liblinear' to see if they converge more easily for your specific data. Change the code to:\n",
        "\n",
        "models = [\n",
        "       LogisticRegression(random_state=42, solver='lbfgs', max_iter=1000), # changing solver from 'saga' to 'lbfgs'\n",
        "   #...(rest of your models)\n",
        "   ]\n",
        "Use code with caution\n",
        "Should you skip the model?\n",
        "\n",
        "If the warning persists even after trying the above solutions and your accuracy is poor, it might be worth considering skipping the LogisticRegression model with the 'saga' solver, especially if other models perform significantly better.\n",
        "However, if the accuracy is acceptable or comparable to other models despite the warning, it might be okay to keep it in the analysis. You can monitor the warning and make a judgment based on overall model performance.\n",
        "Incorporating the changes\n",
        "\n",
        "Apply the chosen solution(s) to the code snippet where you define the LogisticRegression model within your loop. Remember to track the warning messages and performance metrics to assess the effectiveness of your modifications.\n",
        "\n",
        "I recommend starting with increasing max_iter and then experimenting with adjusting tol if needed. If those don't resolve the issue, consider trying a different solver or potentially skipping the model depending on your accuracy goals.\n",
        "\n",
        "github.com/scikit-learn/scikit-learn/discussions/27062\n",
        "github.com/scikit-learn/scikit-learn/issues/27561\n",
        "stackoverflow.com/questions/46028914/multilayer-perceptron-convergencewarning-stochastic-optimizer-maximum-iterat\n",
        "stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
        "datascience.stackexchange.com/questions/38143/multilayer-perceptron-does-not-converge\n"
      ],
      "metadata": {
        "id": "R2R-6qC208TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I notice that all the iterations of each model tests have the same accuracy , precision, recall and f1. is that expected?\n",
        "\n",
        "Gemini\n",
        "Okay, let's address the observation that all iterations of each model test have the same accuracy, precision, recall, and F1 score. This is not expected in general, and there might be a few reasons why you're seeing this behavior in your code.\n",
        "\n",
        "Possible Reasons for Identical Results Across Iterations\n",
        "Random State and Feature Sampling:\n",
        "\n",
        "The random_state parameter in your models and train_test_split function is set to a fixed value (42). This ensures that the data splitting and model initialization are consistent across iterations.\n",
        "However, the most likely cause is that even though you are sampling features randomly, the features you're selecting are highly correlated with each other or with the target variable. This means that even with different feature sets, the models are essentially learning the same patterns, leading to similar performance.\n",
        "Solution: To confirm if this is the case, you can try the following:\n",
        "Increase the number of features sampled: This might introduce more variability into the models.\n",
        "Check for feature correlations: Examine the correlations between the features you're sampling and with the target variable. High correlations might indicate redundancy in your feature sets.\n",
        "Try different random states: Experiment with different values for random_state to see if the results vary.\n",
        "Data Leakage or Overfitting:\n",
        "\n",
        "Data leakage: If information from the test set is unintentionally leaking into the training process (e.g., through feature engineering or data preprocessing), it can lead to overoptimistic and consistent results across iterations.\n",
        "Overfitting: If your models are overfitting to the training data, they might perform well on the training set but poorly on unseen data. This could lead to similar scores across iterations, as the models are essentially memorizing the training data instead of learning generalizable patterns.\n",
        "Solution: To address data leakage and overfitting:\n",
        "Carefully review your data preprocessing steps: Ensure that you're not using information from the test set during training.\n",
        "Use techniques to mitigate overfitting: Consider using regularization, cross-validation, or simpler models to prevent overfitting.\n",
        "Class Imbalance:\n",
        "\n",
        "If your target variable has a very imbalanced distribution of classes (e.g., one class is much more frequent than the others), it can make the models overly biased towards the majority class. This can result in similar accuracy scores but poor performance on the minority class, which might not be reflected in the overall metrics.\n",
        "Solution: To address class imbalance:\n",
        "Use techniques like oversampling, undersampling, or cost-sensitive learning to balance the class distribution during training.\n",
        "In Your Specific Case\n",
        "Given that you're using randomly sampled features, it's most likely that the chosen features have similar predictive power, leading to consistent results across iterations. However, the fact that the same data is being used for training and testing for each target variable further suggests that the model may not have fully learned the expected behaviour of each target variable. In this particular case, the similar performance is expected due to limitations inherent to using randomly selected features and the dataset construction, where correlations between features and targets are pre-defined, reducing the impact of feature variation on model performance.\n",
        "\n",
        "I recommend reviewing the feature correlations and considering using feature selection or engineering techniques to potentially identify more relevant features for each target variable to improve model learning."
      ],
      "metadata": {
        "id": "qY3auu9YvN4x"
      }
    },
    {
      "source": [
        "results_df.to_csv('results.csv', index=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NSqNMskgPOVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrxNQBl1QsEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pickle\n",
        "with open('INFLATION_MAJOR_COMMIT_RandomForestClassifier_3.pkl', 'rb') as file:  # Replace 'your_pickle_file.pkl' with the actual filename\n",
        "       data = pickle.load(file)\n",
        "       print(data)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CMOynIUfQsoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('INFLATION_MAJOR_COMMIT_SVC.pkl', 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "print(loaded_data)"
      ],
      "metadata": {
        "id": "FoQX-4l1RLk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tHE6-n8o1E4e"
      }
    }
  ]
}